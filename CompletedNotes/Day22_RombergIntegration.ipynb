{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMK/CMJSAWFwSMB50UDmM/9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XqUojMORuaJV"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# Day 22: Romberg Integration\n","\n","In the previous notebook we developed, implemented, and utilized the *Trapezoidal Method* for approximating $\\displaystyle{\\int_{a}^{b}{f\\left(x\\right)dx}}$. In particular, that method states\n","\n","$$\\int_{a}^{b}{f\\left(x\\right)dx} = \\sum_{i = 0}^{n-1}{\\frac{f\\left(x_i\\right) + f\\left(x_{i+1}\\right)}{2}h}$$\n","\n","where $\\displaystyle{h = \\frac{b - a}{n}}$ is the width of each sub-interval.\n","\n","*Romberg Integration* combines the *Trapezoidal Method* with *Richardson Extrapolation* for improved efficiency. Before discussing and implementing *Romberg Integration*, let's remind ourselves of the *Trapezoidal Method* and *Richardson Extrapolation*."],"metadata":{"id":"KIb4fA7nzmrF"}},{"cell_type":"markdown","source":["### A Reminder on the Trapezoidal Method\n","\n","When we implemented `trapezoidalMethod()`, we used a recursive approach to approximate the integral. This recursive approach did not require any additional computations, but it did allow for the tracking of differences in approximate areas between consecutive iterations. This gives us an advantage in that we can measure the convergence of the process.\n","\n","For convenience, our implementation of the recursive *Trapezoidal Method* is below."],"metadata":{"id":"U_z_xkCCuex0"}},{"cell_type":"code","source":["def trapezoid(f, a, b, Iold, k):\n","  if k == 1:\n","    Inew = (f(a) + f(b))*(b - a)/2.0\n","  else:\n","    n = 2**(k-2)\n","    h = (b - a)/n\n","    x = a + h/2.0\n","    sum = 0.0\n","    for i in range(n):\n","      sum = sum + f(x)\n","      x = x + h\n","\n","    Inew = (Iold + h*sum)/2.0\n","\n","  return Inew\n","\n","def trapezoidMethod(f, a, b, k):\n","  Iold = (f(a) + f(b))*(b - a)/2.0\n","\n","  for i in range(2, k + 1):\n","    Inew = trapezoid(f, a, b, Iold, i)\n","    diff = Inew - Iold\n","    #print(\"The difference in consecutive estimates was \", diff)\n","    Iold = Inew\n","\n","  return Inew"],"metadata":{"id":"DEXCZIpJueKF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example:** We can use our `trapezoidMethod()` function to compute the area underneath $\\displaystyle{f\\left(x\\right) = \\left|\\sin\\left(3x\\right)\\right|e^x}$ along $\\left[0, 3\\right]$."],"metadata":{"id":"SYlV4xFJwk3T"}},{"cell_type":"code","source":[],"metadata":{"id":"R-QJiiDPD4nh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","def f(x):\n","  return np.abs(np.sin(3*x)*np.exp(x))\n","\n","x_vals = np.linspace(0, 3, 250)\n","y_vals = f(x_vals)\n","\n","k = 4\n","\n","x_abscissas = np.linspace(0, 3, 2**(k - 1) + 1)\n","y_abscissas = f(x_abscissas)\n","\n","plt.figure(figsize = (12, 4))\n","plt.plot(x_vals, y_vals, color = \"purple\")\n","plt.plot(x_abscissas, y_abscissas, color = \"red\")\n","plt.scatter(x_abscissas, y_abscissas, color = \"red\", s = 25)\n","plt.vlines(x_abscissas, 0, y_abscissas, color = \"red\", ls = \"--\")\n","plt.fill_between(x_abscissas, 0, y_abscissas, color = \"red\", alpha = 0.2)\n","\n","plt.grid()\n","plt.axvline(x = 0, color = \"black\")\n","plt.axhline(y = 0, color = \"black\")\n","plt.xlabel(\"x\")\n","plt.ylabel(\"y\")\n","plt.title(\"Trapezoidal Method to Estimate f(x) = |sin(3x)|e^x on [0, 3]\")\n","plt.show()\n","\n","est_area = trapezoidMethod(f, 0, 3, k)\n","\n","print(\"The estimated area using \" + str(2**(k-1)) + \" subintervals is \" + str(est_area))"],"metadata":{"id":"5bPlWgqZwa_8","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### A Reminder on Richardson Extrapolation\n","\n","The *Richardson Extrapolation* technique is a method used to eliminate or reduce error in approximations. As a reminder, the method assumes that the error in approximating a quantity $G$ is $\\displaystyle{E = ch^p}$ for some constants $c$ and $p$. The constant $p$ is usually obtained by examining the *order* of the *truncation error*. For the majority of our approaches that has been $\\mathcal{O}\\left(h^2\\right)$.\n","\n","To use the technique, we obtain two estimates for $G$ using different $h$ values (interval widths). It is common to use $h_2 = h_1/2$. In doing this, we obtain\n","\n","$$G = \\frac{2^pg\\left(h_1/2\\right) - g\\left(h_1\\right)}{2^p - 1}$$\n","\n","where $g\\left(h_1\\right)$ and $g\\left(h_1/2\\right)$ are the two estimates obtained for $G$."],"metadata":{"id":"84L3uyxl0DE1"}},{"cell_type":"markdown","source":["## Romberg Integration\n","\n","*Romberg Integration* combines the *Trapezoidal Method* with *Richardson Extrapolation* to improve the efficiency in using the *Trapezoidal Method* to approximate $\\displaystyle{\\int_{a}^{b}{f\\left(x\\right)dx}}$.\n","\n","We begin with the notation $R_{j,1} = I_j$ where $I_j$ represents the approximation for the integral using $2^{j-1}$ subintervals (as we did in the previous notebook and our implementation of the *recursive trapezoidal method*). As mentioned at the end of the previous notebook, the error in this approximation is $E = c_1h^2 + c_2h^4 + \\dots$, where $\\displaystyle{h = \\frac{b-a}{2^{j-1}}}$\n","\n","We'll start with $R_{1,1} = I_1$ and $R_{2, 1} = I_2$ -- estimates using one and two subintervals, respectively.\n","\n","\\begin{align*}R_{1, 1} &= \\left[f\\left(a\\right) + f\\left(b\\right)\\right]\\frac{H}{2}\\\\\n","R_{2,1} &= \\frac{1}{2}R_{1,1} + f\\left(a + \\frac{H}{2}\\right)\\frac{H}{2}\n","\\end{align*}\n","\n","Using *Richardson Extrapolation*, we have\n","\n","\\begin{align*} \\int_{a}^{b}{f\\left(x\\right)dx} &\\approx \\frac{2^2R_{2,1} - R_{1,1}}{2^2 - 1}\\\\\n","&= \\frac{4}{3}R_{2,1} - \\frac{1}{3}R_{1,1}\n","\\end{align*}\n","\n","We then label $\\displaystyle{R_{2,2} = \\frac{4}{3}R_{2,1} - \\frac{1}{3}R_{1,1}}$, the result of using *Richardson Extrapolation* to eliminate the leading error term ($c_1h^2$). We can organize the results into an array of the form:\n","\n","$$R = \\left[\\begin{array}{cc} R_{1,1} & \\\\ R_{2,1} & R_{2,2}\\end{array}\\right]$$\n","\n","Next we compute $R_{3,1}$, $R_{3,2}$ and $R_{3,3}$. Note that $R_{3,1} = I_3$ -- the estimate for the integral with $2^{3-1} = 4$ sub-intervals. We can then use *Richardson Extrapolation* to obtain\n","\n","$$R_{3,2} = \\frac{4}{3}R_{3,1} - \\frac{1}{3}R_{2,1}$$\n","\n","resulting in the array\n","\n","$$R = \\left[\\begin{array}{ccc} R_{1,1} &  & \\\\ R_{2,1} & R_{2,2} & \\\\ R_{3,1} & R_{3, 2} & \\end{array}\\right]$$\n","\n","The two elements in the second column are estimates with errors whose leading term is of the form $ch^4$, so we can use *Richardson Extrapolation* to eliminate that leading error term. In doing so, we obtain\n","\n","\\begin{align*} R_{3,3} &= \\frac{2^4R_{3,2} - R_{2,2}}{2^4 - 1}\\\\\n","&= \\frac{16}{15}R_{3,2} - \\frac{1}{15}R_{2,2}\n","\\end{align*}\n","\n","Note that the dominant error term for $R_{3,3}$ is $\\mathcal{O}\\left(h^6\\right)$. The resulting array $R$ is now\n","\n","$$R = \\left[\\begin{array}{ccc} R_{1,1} &  & \\\\ R_{2,1} & R_{2,2} & \\\\ R_{3,1} & R_{3,2} & R_{3,3}\\end{array}\\right]$$\n","\n","We can continue in this fashion, expanding the array $R$ with more accurate approximations of $\\displaystyle{\\int_{a}^{b}{f\\left(x\\right)dx}}$ appearing in the lower-right corner. We continue the process until the difference in diagonal elements is sufficiently small (this means that the approximation for the area is converging).\n","\n","The general form for computing $R_{i,j}$ appears below:\n","\n","$$R_{i,j} = \\frac{4^{j-1}R_{i, j-1} - R_{i-1, j-1}}{4^{j-1} - 1}$$"],"metadata":{"id":"9zwsjpz51SOt"}},{"cell_type":"markdown","source":["### Implementing Romberg Integration\n","\n","The 2-D array we used in the previous subsection is convenient for our own understanding and for \"by-hand\" computations. A computer, however, can run *Romberg Integration* using just a 1-D array. As a reminder, space- and run-time efficiency are extremely important considerations in numerical analysis. Usually we are encoding routines because we would like to solve lots of problems very quickly -- perhaps to observe change over short time intervals or to inform real-time decisions made by a system.\n","\n","To carry out *Romberg Integration* with a 1-D array, recall that computing $R_{i,j}$ depends only on $R_{i, j-1}$ and $R_{i-1, j-1}$. For example, $R_{1,1}$ is never used after $R_{2,2}$ is calculated. This means that after computing $R_{2,2}$ we can get away with just the array\n","\n","$$R' = \\left[\\begin{array}{c} R_1' = R_{2,2}\\\\ R_2' = R_{2,1}\\end{array}\\right]$$\n","\n","Once we compute $R_{3,2}$, we no longer need $R_{2,1}$, so we can replace $R_2'$ with $R_{3,2}$. Similarly we can replace $R_{2,2}$ with $R_{3,3}$. That is,\n","\n","$$R' = \\left[\\begin{array}{c} R_1' = R_{3,3}\\\\ R_2' = R_{3,2}\\\\ R_3' = R_{3,1}\\end{array}\\right]$$\n","\n","In general, the array $R'$ at the $k^{th}$ round is given by\n","\n","$$R_j' = \\frac{4^{k-j}R_{j+1}' - R_j'}{4^{k-j} - 1}~~\\text{for}~~j = k - 1, k - 2, \\cdots, 1$$\n","\n","We are now ready to write our `rombergIntegration()` routine."],"metadata":{"id":"BKoH9kwM8djE"}},{"cell_type":"code","source":["def richardsonExtrapolation(r, k):\n","  for j in range(k - 1, 0, -1):\n","    const = 4.0**(k-j)\n","    r[j] = (const*r[j+1] - r[j])/(const - 1.0)\n","\n","  return r\n","\n","def rombergIntegration(f, a, b, tol = 1.0e-6):\n","  r = np.zeros(21)\n","  r[1] = trapezoid(f, a, b, Iold = 0.0, k = 1)\n","\n","  r_old = r[1]\n","\n","  for k in range(2, len(r)):\n","    r[k] = trapezoidMethod(f, a, b, k)\n","    r = richardsonExtrapolation(r, k)\n","    if abs(r[1] - r_old) < tol*max(abs(r[1]), 1.0):\n","      return r[1], 2**(k-1)\n","\n","    r_old = r[1]\n","\n","  print(\"Romberg Integration did not converge.\")\n","  return None"],"metadata":{"id":"RIpqmsfxyI_x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example 6.6:** Use Romberg Integration to evaluate $\\displaystyle{\\int_{0}^{\\pi}{\\sin\\left(x\\right)dx}}$ to four decimal places.\n","\n","> *Solution.*"],"metadata":{"id":"gqzO-b_6AOcp"}},{"cell_type":"code","source":[],"metadata":{"id":"WledFyyHAKFl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example 6.7:** Use Romberg Integration to evaluate $\\displaystyle{\\int_{0}^{\\sqrt{\\pi}}{2x^2\\cos\\left(x^2\\right)dx}}$. Compare the result to the answer for ***Example 6.4***. Notice how much more quickly the convergence occurs!\n","\n","> *Solution.*"],"metadata":{"id":"la647a_CBO_n"}},{"cell_type":"code","source":[],"metadata":{"id":"bQ6N4ZiDAkQg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***\n","\n","## Summary\n","\n","In this notebook we introduced, implemented, and utilized Romberg Integration to observe really strong improvements in efficiency over the Trapezoidal Method for approximating $\\displaystyle{\\int_{a}^{b}{f\\left(x\\right)dx}}$. In particular, **Example 6.7** converged at only 64 subintervals with Romberg Integration versus 4,096 required for the classic Trapezoidal Method. This improved efficiency can make a huge difference if we need to solve problems very quickly to help a system make real-time decisions."],"metadata":{"id":"_c95r1j4B5CI"}}]}