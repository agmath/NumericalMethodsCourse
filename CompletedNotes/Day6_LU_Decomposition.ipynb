{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMNnFK4CbOJPEt9vOrnL+vL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"fM0B-snPdoVT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Day 6: LU Decomposition\n","\n","In the previous notebook we wrote a routine to solve linear systems of the form $A\\vec{x} = \\vec{b}$ using *Gaussian Elimination*. That approach uses elementary row operations to reduce the matrix $A$ to an *upper triangular matrix* $U$ and then uses back-substitution to solve the system $U\\vec{x} = \\vec{c}$. A major drawback to the Gaussian Elimination approach is that the procedure must begin from scratch in solving $A\\vec{x} = \\vec{b'}$. This is not very efficient if we desire to solve $A\\vec{x} = \\vec{b}$ for many different constant vectors $\\vec{b}$.\n","\n","The *LU decomposition* approach rewrites the system $A\\vec{x} = \\vec{b}$ as $LU\\vec{x} = \\vec{b}$ -- notice that the constant vector is left unchanged since we are just decomposing $A$ as the product of a *lower triangular matrix* $L$ and an *upper triangular matrix* $U$."],"metadata":{"id":"BQz5Z4YrCDfU"}},{"cell_type":"markdown","source":["### How to Solve $LU\\vec{x}=\\vec{b}$\n","\n","Solving a system $LU\\vec{x} = \\vec{b}$ is quite straight forward.\n","\n","1. Replace $U\\vec{x} = \\vec{y}$ and use *forward-substitution* to solve $L\\vec{y} = \\vec{b}$.\n","2. Obtain the solution to the system by using *back-substitution* $U\\vec{x} = \\vec{y}$.\n","\n","Let's see this in a small example.\n","\n","**Example:** Consider the system of the form $LU\\vec{x} = \\vec{b}$ where $L = \\left[\\begin{array}{cc} 2 & 0\\\\ 1 & -3\\end{array}\\right]$, $U = \\left[\\begin{array}{cc} 1 & -1\\\\ 0 & 2\\end{array}\\right]$, and $\\vec{b} = \\left[\\begin{array}{c} 16\\\\ 38\\end{array}\\right]$.\n","> *Solution.* We begin by solving $L\\vec{y} = \\vec{b}$. That is,\n",">\n","> $$\\left[\\begin{array}{cc} 2 & 0\\\\ 1 & -3\\end{array}\\right]\\left[\\begin{array}{c} y_1\\\\ y_2\\end{array}\\right] = \\left[\\begin{array}{c} 16\\\\ 38\\end{array}\\right]$$\n",">\n","> We can use forward substitution here.\n",">\n","> \\begin{align*} 2y_1 &= 16\\\\\n","\\implies y_1 &= 8~\\checkmark\\\\\n","y_1 - 3y_2 &= 38\\\\\n","\\implies 8 -3y_2 &= 38\\\\\n","\\implies -3y_2 &= 30\\\\\n","\\implies y_2 &= -10~\\checkmark\n","\\end{align*}\n",">\n","> That is, $\\vec{y} = \\left[\\begin{array}{c} 8\\\\ -10\\end{array}\\right]$. Now we'll solve $U\\vec{x} = \\vec{y}$. That is,\n",">\n","> $$\\left[\\begin{array}{cc} 1 & -1\\\\ 0 & 2\\end{array}\\right]\\left[\\begin{array}{c} x_1\\\\ x_2\\end{array}\\right] = \\left[\\begin{array}{c} 8\\\\ -10\\end{array}\\right]$$\n",">\n","> We can use backward substitution here.\n",">\n","> \\begin{align*} 2x_2 &= -10\\\\\n","\\implies x_2 &= -5~\\checkmark\\\\\n","x_1 - x_2 &= 8\\\\\n","\\implies x_1 - \\left(-5\\right) &= 8\\\\\n","\\implies x_1 &= 3~\\checkmark\n","\\end{align*}\n",">\n","> That is, $~\\boxed{\\vec{x} = \\left[\\begin{array}{c} 3\\\\ -5\\end{array}\\right]~}$."],"metadata":{"id":"yLlYo6svFOY2"}},{"cell_type":"markdown","source":["## Common Decomposition Methods\n","\n","While not every square matrix $A$ has an $LU$ decomposition, it is the case that for any given square matrix $A$, it is always possible to find a *lower triangular matrix* $L$ and an *upper triangular matrix* $U$ such that $LU$ is row-equivalent to $A$. In fact, there are infinitely many pairs $L$ and $U$ which can be used to rewrite that matrix. For this reason, some common methods with constraints have been developed.\n","\n","<center>\n","\n","Name | Constraints\n","---|---\n","Doolittle's Decomposition | $L_{ii} = 1$ for $i\\in[n]$\n","Crout's Decomposition | $U_{ii} = 1$ for $i\\in [n]$\n","Choleski's Decomposition | $L = U^T$\n","\n","</center>\n","\n","The methods of Doolittle and Crout are nearly identical since the only difference is which main diagonal is restricted to all $1$'s. For this reason, we'll only discuss Doolittle and Choleski's methods in this notebook.\n","\n"],"metadata":{"id":"gQM29LtmDtSZ"}},{"cell_type":"markdown","source":["## Dolittle's Decomposition Method\n","\n","As with the *Gaussian Elimination* routine, this method of solution will also come in two phases -- the *decomposition phase* and the *solution phase*. In addition to talking through the *decomposition phase*, we'll use an example to help us see what is happening."],"metadata":{"id":"v_BSgPadJl-y"}},{"cell_type":"markdown","source":["### Decomposition Phase\n","\n","Let's consider a square matrix $A = \\left[\\begin{array}{ccc} A_{11} & A_{12} & A_{13}\\\\\n","A_{21} & A_{22} & A_{23}\\\\\n","A_{31} & A_{32} & A_{33}\\end{array}\\right]$ and assume that there exist $L = \\left[\\begin{array}{ccc} 1 & 0 & 0\\\\\n","L_{21} & 1 & 0\\\\\n","L_{31} & L_{32} & 1\\end{array}\\right]$ and $U = \\left[\\begin{array}{ccc} U_{11} & U_{12} & U_{13}\\\\\n","0 & U_{22} & U_{23}\\\\\n","0 & 0 & U_{33}\\end{array}\\right]$ such that $A = LU$. Since $A = LU$, we have\n","\n","$$A = \\left[\\begin{array}{ccc} U_{11} & U_{12} & U_{13}\\\\\n","U_{11}L_{21} & U_{12}L_{21} + U_{22} & U_{13}L_{21} + U_{23}\\\\\n","U_{11}L_{31} & U_{12}L_{31} + U_{22}L_{32} & U_{13}L_{31} + U_{23}L_{32} + U_{33}\\end{array}\\right]$$\n","\n","From here, we could apply *Gaussian Elimination* to help us solve for the entries $U_{ij}$ and $L_{ij}$. The reduced form of the matrix above is:\n","\n","\\begin{align*} \\left[\\begin{array}{ccc} U_{11} & U_{12} & U_{13}\\\\\n","U_{11}L_{21} & U_{12}L_{21} + U_{22} & U_{13}L_{21} + U_{23}\\\\\n","U_{11}L_{31} & U_{12}L_{31} + U_{22}L_{32} & U_{13}L_{31} + U_{23}L_{32} + U_{33}\\end{array}\\right] &\\stackrel{R_2 \\leftarrow R_2 - L_{21}R_1}{\\to} \\left[\\begin{array}{ccc} U_{11} & U_{12} & U_{13}\\\\\n","0 & U_{22} & U_{23}\\\\\n","U_{11}L_{31} & U_{12}L_{31} + U_{22}L_{32} & U_{13}L_{31} + U_{23}L_{32} + U_{33}\\end{array}\\right]\\\\\n","&\\stackrel{R_3 \\leftarrow R_3 - L_{31}R_1}{\\to} \\left[\\begin{array}{ccc} U_{11} & U_{12} & U_{13}\\\\\n","0 & U_{22} & U_{23}\\\\\n","0 & U_{22}L_{32} &U_{23}L_{32} + U_{33}\\end{array}\\right]\\\\\n","&\\stackrel{R_3 \\leftarrow R_3 - L_{32}R_2}{\\to} \\left[\\begin{array}{ccc} U_{11} & U_{12} & U_{13}\\\\\n","0 & U_{22} & U_{23}\\\\\n","0 & 0 & U_{33}\\end{array}\\right]\n","\\end{align*}\n","\n","An few interesting things have occurred here!\n","\n","+ The matrix $U$ in the $LU$ decomposition is simply the *upper triangular matrix* resulting from Gaussian Elimination.\n","+ The off-diagonal elements of the matrix $L$ are the scalar multipliers for the pivot rows used during the row reduction.\n"],"metadata":{"id":"xxkoETwaKW_h"}},{"cell_type":"markdown","source":["### An Example to Confirm...\n","\n","Let's confirm what we saw above with a small example.\n","\n","**Example:** Consider the matrix $A = \\left[\\begin{array}{ccc} 1 & 0 & 1\\\\\n","2 & -1 & 5\\\\\n","3 & 3 & 3\\end{array}\\right]$. Use Gaussian Elimination to row reduce the matrix and then use the result (and the pivot multipliers) to construct the Doolittle $LU$-decomposition of $A$.\n","> *Solution.*\n","> \\begin{align*} \\left[\\begin{array}{ccc} 1 & 0 & 1\\\\\n","2 & -1 & 5\\\\\n","3 & 3 & 3\\end{array}\\right] &\\stackrel{R_1 \\leftarrow R_2 - 2R_1}{\\to} \\left[\\begin{array}{ccc} 1 & 0 & 1\\\\\n","0 & -1 & 3\\\\\n","3 & 3 & 3\\end{array}\\right]\\\\\n","&\\stackrel{R_3 \\leftarrow R_3 - 3R_1}{\\to} \\left[\\begin{array}{ccc} 1 & 0 & 1\\\\\n","0 & -1 & 3\\\\\n","0 & 3 & 0\\end{array}\\right]\\\\\n","&\\stackrel{R_3 \\leftarrow R_3 - (-3)R_2}{\\to} \\left[\\begin{array}{ccc} 1 & 0 & 1\\\\\n","0 & -1 & 3\\\\\n","0 & 0 & 9\\end{array}\\right]\\\\\n","\\end{align*}\n",">\n","> From the above, we've discovered that $U = \\left[\\begin{array}{ccc} 1 & 0 & 1\\\\\n","0 & -1 & 3\\\\\n","0 & 0 & 9\\end{array}\\right]$, and keeping track of the scalar multipliers for our pivots, we have $L_{21} = 2$, $L_{31} = 3$, and $L_{32} = -3$. That is, $U = \\left[\\begin{array}{ccc} 1 & 0 & 0\\\\\n","2 & 1 & 0\\\\\n","3 & -3 & 1\\end{array}\\right]$.\n",">\n","> We'll now explicitly show that $LU = A$.\n",">\n","> \\begin{align*}LU &= \\left[\\begin{array}{ccc} 1 & 0 & 0\\\\\n","2 & 1 & 0\\\\\n","3 & -3 & 1\\end{array}\\right]\\left[\\begin{array}{ccc} 1 & 0 & 1\\\\\n","0 & -1 & 3\\\\\n","0 & 0 & 9\\end{array}\\right]\\\\\n","&= \\left[\\begin{array}{ccc} 1 & 0 & 1\\\\\n","2 & -1 & 5\\\\\n","3 & 3 & 3\\end{array}\\right]\n","\\end{align*}"],"metadata":{"id":"4Qfwc0gFUbLz"}},{"cell_type":"markdown","source":["### A Space-Saving Trick\n","\n","Since the *lower-* and *upper-triangular* matrices $L$ and $U$ have no overlapping non-zero (or non-one) elements, we can store $L$ and $U$ compactly within one data structure. This can be important as we solve very large problems and actual computer memory becomes a concern.\n","\n","Note that the structure $\\left[L/U\\right] = \\left[\\begin{array}{ccc} U_{11} & U_{12} & U_{13}\\\\\n","L_{21} & U_{22} & U_{23}\\\\\n","L_{31} & L_{32} & U_{33}\\end{array}\\right]$ contains all of the information required to reconstruct the matrices $L$ and $U$. Indeed,\n","\n","+ we know all of the elements of $U$ below the main diagonal are $0$, and\n","+ all of the elements of $L$ along the main diagonal are $1$ while all of the elements above it are $0$.\n","\n","**Example:** If $\\left[L/U\\right] = \\left[\\begin{array}{ccc} 2 & 5 & -6\\\\\n","1 & -4 & 3\\\\\n","9 & 7 & 8\\end{array}\\right]$, then we know $L = \\left[\\begin{array}{ccc} 1 & 0 & 0\\\\\n","1 & 1 & 0\\\\\n","9 & 7 & 1\\end{array}\\right]$ and $U = \\left[\\begin{array}{ccc} 2 & 5 & -6\\\\\n","0 & -4 & 3\\\\\n","0 & 0 & 8\\end{array}\\right]$."],"metadata":{"id":"HrssZFvZXnAA"}},{"cell_type":"markdown","source":["## Doolittle's LU Decomposition Algorithm\n","\n","Since we've already written some code to solve a system via backward substitution, we'll be able to adapt that code to run forward substitution easily. We won't walk through it explicitly.\n","\n","Notice that the algorithm to decompose $A$ into its $LU$-decomposition is simply the Gaussian Elimination algorithm. The only additional step is that we'll need to save those $\\lambda$ values (the scalar multipliers for our pivots) along the way. We'll also use the space-saving strategy from above to pack $L$ and $U$ into a single data structure.\n","\n","```\n","#decomposition routine\n","for k in range(0, n-1):\n","  for i in range(k+1, n):\n","    if A[i, k] != 0.0:\n","      lam = A[i, k]/A[k, k]\n","      A[i, (k+1):n] = A[i, (k+1):n] - lam*A[k, (k+1):n]\n","      A[i, k] = lam\n","```\n","\n","The array $A$ resulting from the decomposition routine above will contain the entries of $L$ below the main diagonal, and the entries of $U$ on and above the main diagonal.\n","\n","As a reminder, the back-substitution routine for solving a system whose coefficient matrix is upper-triangular is given below:\n","\n","```\n","#back-substitution routine\n","for k in range(n - 2, -1, -1):\n","  b[k] = (b[k] - np.dot(A[k, (k+1):n], b[(k+1):n]))/A[k, k]\n","```\n","\n","Similarly, the forward substitution routine is:\n","\n","```\n","#forward-substitution routine\n","for k in range(1, n):\n","  b[k] = (b[k] - np.dot(A[k, 0:k], b[0:k]))/A[k, k]\n","```\n","\n","We can put these things together into the `DoolittleLUdecomp()` routine below."],"metadata":{"id":"fnN0chCZZUk3"}},{"cell_type":"code","source":["def LUdecomp(A):\n","  n = A.shape[0]\n","  for k in range(n):\n","    for i in range(k+1, n):\n","      if A[i, k] != 0.0:\n","        lam = A[i, k]/A[k, k]\n","        A[i, (k+1):n] = A[i, (k+1):n] - lam*A[k, (k+1):n]\n","        A[i, k] = lam\n","\n","  return A\n","\n","def LUsolve(LU, b):\n","  n = len(b)\n","  for k in range(1, n):\n","    b[k] = (b[k] - np.dot(LU[k, 0:k], b[0:k]))\n","\n","  b[n-1] = b[n-1]/LU[n-1, n-1]\n","\n","  for k in range(n-2, -1, -1):\n","    b[k] = (b[k] - np.dot(LU[k, (k+1):n], b[(k+1):n]))/LU[k, k]\n","\n","  return b\n","\n","def DoolittleLUsolver(A, b):\n","  LU = LUdecomp(A)\n","  sol = LUsolve(LU, b)\n","\n","  return sol"],"metadata":{"id":"3AWtOtwEKWVw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example:** Use the `DoolittleLUsolver()` to solve the system $\\left[\\begin{array}{ccc} 1 & 0 & 1\\\\\n","2 & -1 & 5\\\\\n","3 & 3 & 3\\end{array}\\right]\\left[\\begin{array}{c} x_1\\\\ x_2\\\\ x_3\\end{array}\\right] = \\left[\\begin{array}{c} 1\\\\ 3\\\\ 1\\end{array}\\right]$."],"metadata":{"id":"KI_fnr2p3U4a"}},{"cell_type":"code","source":[],"metadata":{"id":"PHGXB-mwxqNm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** Similar to the `GaussElim()` function we wrote last time, the matrix $A$ and the constant vector $\\vec{b}$ are altered when we run our `DoolittleLUsolver()` routine. Beware of this when checking your results or solving another system!"],"metadata":{"id":"cPIexWxzk-eJ"}},{"cell_type":"markdown","source":["**Example:** Use Doolittle's decomposition method to solve $A\\vec{x} = \\vec{b}$ where\n","\n","$$A = \\left[\\begin{array}{rrr} -3 & 6 & -4\\\\ 9 & -8 & 24\\\\ -12 & 24 & -26\\end{array}\\right]~~~~\\text{ and }~~~~ \\vec{b} =\\left[\\begin{array}{r} -3\\\\ 65\\\\ -42\\end{array}\\right]$$\n","\n","> *Solution.*"],"metadata":{"id":"RazLghWiGNsf"}},{"cell_type":"code","source":[],"metadata":{"id":"JRVbuV68GRWX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Choleski's Decomposition Method\n","\n","As a reminder, Choleski's method assumes that $L = U^T$. This means that, Choleski's method assumes that we can rewrite $A =LU = LL^T$. There are two limitations to Choleski's Method for decomposition (and thus using it to solve linear systems).\n","\n","+ The matrix $LL^T$ is always symmetric, so this method can only be applied to scenarios when $A$ is a ***symmetric matrix***.\n","+ The decomposition involves taking square roots of combinations of elements of $A$. It can be shown that $A$ must be ***positive definite*** for the decomposition to be possible.\n","\n","The advantage to Choleski's method is that it is nearly $2\\times$ faster than other $LU$ decompositions that don't utilize the symmetric property of such a coefficient matrix.\n","\n","We won't implement Choleski's method here, but the implementation for it is contained on page 50 of our textbook."],"metadata":{"id":"2uf2rK4Y3yFU"}},{"cell_type":"code","source":[],"metadata":{"id":"keZto8aIy4X3"},"execution_count":null,"outputs":[]}]}