---
title: "MAT 370: Least Squares Regression"
author: Dr. Gilbert
format: 
  revealjs:
    smaller: true
date: today
date-format: long
theme: serif
incremental: true
execute: 
  error: true
---

```{r global-options, include=FALSE}
library(tidyverse)
library(reticulate)

theme_set(theme_bw(base_size = 20))
```

```{python}
import numpy as np
import matplotlib.pyplot as plt
import time

def GaussElim(A, b):
  n = len(b)

  for k in range(0, n-1):
    for i in range(k+1, n):
      if A[i, k] != 0.0:
        lam = A[i, k]/A[k, k]
        A[i, k:n] = A[i, k:n] - lam*A[k, k:n]
        b[i] = b[i] - lam*b[k]

  for k in range(n - 1, -1, -1):
    b[k] = (b[k] - np.dot(A[k, (k+1):n], b[(k+1):n]))/A[k,k]

  return b

def LUdecomp(A):
  n = A.shape[0]
  for k in range(n):
    for i in range(k+1, n):
      if A[i, k] != 0.0:
        lam = A[i, k]/A[k, k]
        A[i, (k+1):n] = A[i, (k+1):n] - lam*A[k, (k+1):n]
        A[i, k] = lam

  return A

def LUsolve(LU, b):
  n = len(b)
  for k in range(1, n):
    b[k] = (b[k] - np.dot(LU[k, 0:k], b[0:k]))

  b[n-1] = b[n-1]/LU[n-1, n-1]

  for k in range(n-2, -1, -1):
    b[k] = (b[k] - np.dot(LU[k, (k+1):n], b[(k+1):n]))/LU[k, k]

  return b

def DoolittleLUsolver(A, b):
  LU = LUdecomp(A) #Row Reduction
  sol = LUsolve(LU, b) #Forward/backward substitution

  return sol

def DoolittleLUdecomp3(c, d, e):
  n = len(d)
  for k in range(1, n):
    lam = c[k-1]/d[k-1]
    d[k] = d[k] - lam*e[k-1]
    c[k-1] = lam

  return c, d, e

def Doolittle3solver(lam, d, e, b):
  n = len(d)
  for k in range(1, n):
    b[k] = b[k] - lam[k-1]*b[k-1]

  b[n-1] = b[n-1]/d[n-1]
  for k in range(n-2, -1, -1):
    b[k] = (b[k] - e[k]*b[k+1])/d[k]

  return b

def DoolittleLUdecomp3solver(c, d, e, b):
  lam, d, e = DoolittleLUdecomp3(c, d, e)
  b = Doolittle3solver(lam, d, e, b)

  return b

def swapRows(v, i, j):
  if len(v.shape) == 1:
    v[i], v[j] = v[j], v[i]
  else:
    v[[i, j], :] = v[[j, i], :]

def scaleFactors(A):
  n = A.shape[0]
  s = np.zeros(n)
  for i in range(n):
    s[i] = max(np.abs(A[i, :]))

  return s

def gaussPivot(A, b, tol = 1.0e-12):
  n = len(b)
  s = scaleFactors(A)

  for k in range(0, n-1):
    p = np.argmax(np.abs(A[k:n, k])/s[k:n]) + k
    if(abs(A[p, k]) < tol):
      print("Matrix is Singular")
      return None

    #Row Pivot if necessary
    if p != k:
      swapRows(b, k, p)
      swapRows(s, k, p)
      swapRows(A, k, p)

    #Elimination
    for i in range(k+1, n):
      if A[i, k] != 0.0:
        lam = A[i, k]/A[k, k]
        A[i, (k+1):n] = A[i, (k+1):n] - lam*A[k, (k+1):n]
        b[i] = b[i] - lam*b[k]

  if abs(A[n-1, n-1]) < tol:
    print("Matrix is Singular")
    return None

  #back substitution
  b[n-1] = b[n-1]/A[n-1, n-1]
  for k in range(n-2, -1, -1):
    b[k] = (b[k] - np.dot(A[k, (k+1):n], b[(k+1):n]))/A[k, k]

  return b

def getCoefficients(xData, yData):
  m = len(xData)
  a = yData.copy()

  for k in range(1, m):
    a[k:m] = (a[k:m] - a[k-1])/(xData[k:m] - xData[k-1])

  return a

def evaluatePolynomial(a, xData, x):
  deg = len(xData) - 1
  p = a[deg]

  for k in range(1, deg + 1):
    p = a[deg-k] + (x - xData[deg-k])*p

  return p

def curvatures(xData, yData):
  n = len(xData) - 1
  c = np.zeros(n)
  d = np.ones(n+1)
  e = np.zeros(n)
  k = np.zeros(n+1)

  c[0:(n-1)] = xData[0:(n-1)] - xData[1:n]
  d[1:n] = 2.0*(xData[0:(n-1)] - xData[2:(n+1)])
  e[1:n] = xData[1:n] - xData[2:(n+1)]
  k[1:n] = 6.0*((yData[0:(n-1)] - yData[1:n])/(xData[0:(n-1)] - xData[1:n])) - 6.0*((yData[1:n] - yData[2:(n+1)])/(xData[1:n] - xData[2:(n+1)]))

  #DoolittleLUdecomp3(c, d, e)
  DoolittleLUdecomp3solver(c, d, e, k)

  return k

def findSegment(xData, x):
    iLeft = 0
    iRight = len(xData) - 1

    while 1:
      if (iRight - iLeft) <= 1:
        return iLeft
      i = int((iLeft + iRight)/2)
      if x < xData[i]:
        iRight = i
      else:
        iLeft = i

def evalSpline(xData, yData, k, x):
  i = findSegment(xData, x)
  h = xData[i] - xData[i+1]
  y = ((x - xData[i+1])**3/h - (x - xData[i+1])*h)*k[i]/6.0 - ((x - xData[i])**3/h - (x - xData[i])*h)*k[i+1]/6.0 + (yData[i]*(x - xData[i+1]) - yData[i+1]*(x - xData[i]))/h
  y = ((x - xData[i+1])**3/h - (x - xData[i+1])*h)*k[i]/6.0 - ((x - xData[i])**3/h - (x - xData[i])*h)*k[i+1]/6.0 + (yData[i]*(x - xData[i+1]) - yData[i+1]*(x - xData[i]))/h


  return y
```

```{css}
code.sourceCode {
  font-size: 1.3em;
  /* or try font-size: xx-large; */
}

a {
  color: purple;
}

a:link {
  color: purple;
}

a:visited {
  color: purple;
}
```

## Motivation and Context

<div style="font-size:18pt">

:::{.fragment}

Often we must recognize that relationships are neither deterministic nor causal.

:::

:::{.fragment}

Noisy associations exist between one or more independent variables and a response.

:::

:::{.fragment}

In these cases, building an interpolant doesn't make sense. Instead, we want to capture a general trend.

:::

::::{.columns}

:::{.column width="50%"}

:::{.fragment}

<center>**Interpolant**</center>

```{python}
np.random.seed(370)
x = np.sort(np.random.uniform(0, 10, 25))
y = 3*x + 2 + np.random.normal(0, 5, 25)

x_new = np.linspace(0, 10, 10000)

k = curvatures(x, y)

y_new = x_new.copy()
for j in range(len(x_new)):
  y_new[j] = evalSpline(x, y, k, x_new[j])

plt.scatter(x, y, s = 150)
plt.plot(x_new, y_new, linewidth = 3, color = "purple")
plt.grid()
plt.axhline(color = "black")
plt.axvline(color = "black")
plt.show()
```

:::

:::

:::{.column width="50%"}

:::{.fragment}

<center>**Least Squares Fit**</center>

```{python}
my_fit = np.polyfit(x, y, 1)
y_new = my_fit[1] + (my_fit[0]*x_new)

plt.scatter(x, y, s = 150)
plt.plot(x_new, y_new, linewidth = 3, color = "purple")
plt.grid()
plt.axhline(color = "black")
plt.axvline(color = "black")
plt.show()
```

:::

:::

::::

:::{.fragment}

The interpolant is doing exactly what its designed to do -- pass through every observed data point exactly, but that forces unsupported variability (even with using a cubic spline interpolant). We probably don't realistically expect new data to follow the interpolant -- the model on the right is more conservative, but likely more trustworthy.

:::

</div>

## Overview

<div style="font-size:18pt">

:::{.fragment}

With admittedly noisy data and noisy relationships, we want to build a model that captures a general trend between the availabile independent variables and our response.

:::

:::{.fragment}

That model should have a fairly simple form, otherwise we risk fitting the noise, which is unpredictable by definition.

:::

:::{.fragment}

Consider a function $f\left(x\right) = f\left(x\mid \beta_0, \beta_1,\cdots,\beta_m\right)$ which has been fitted using $n+1$ observed data points of the form $\left(\vec{x}_i, y_i\right)$.

:::

:::{.fragment}

This function includes $m+1$ parameters ($\beta_0, \cdots, \beta_m$), so $m+1 < n+1$. 

:::

:::{.fragment}

The observed points $\left(\vec{x}_i, y_i\right)$ consist of measurements on independent variables which are contained in $\vec{x}_i$ and a corresponding measured dependent response contained in $y_i$. 

:::

+ Note that $\vec{x}_i$ may consist of a single measured variable or many. 
+ For example, if the data represent the *displacements* $y_i$ of an overdamped mass-spring system at time $t$, then the observations are of the form $\left(t_i, y_i\right)$ and the form of the model suggested by theory is $\displaystyle{f\left(t\right) = a_0te^{-a_1t}}$. 

  + The parameters $a_0$ and $a_1$ for the model are to be *learned* from the observed data (these take the place of the $\beta$s referenced above).
  
</div>

## Fitting / Learning Parameters (Intuition)

:::{.fragment}

<div style="font-size:20pt">

In general, parameters for a fitted model are obtained by minimizing a *loss function*. 

</div>

:::

:::{.fragment}

<div style="font-size:20pt">

If we are willing to assume that the *noise* is a feature of the response variable only (and the measurements on the independent variable(s) are to be trusted), then the most common loss function is the *Sum of Squared Errors*:

</div>

:::

:::{.fragment}

<div style="font-size:18pt">

$$L\left(\beta_0, \beta_1,\cdots,\beta_m\right) = \sum_{i=0}^{n}{\left[y_i - f\left(x_i\right)\right]^2}$$

</div>

:::

:::{.fragment}

<div style="font-size:20pt">

Models fit by minimizing the loss function above are said to be fit using *Ordinary Least Squares* (OLS).

</div>

:::

## Fitting / Learning Parameters (Mechanics)

::::{.columns}

:::{.column width="50%"}

:::{.fragment}

<div style="font-size:18pt">

The values of the $\beta$-parameters minimizing any Loss Function will satisfy the simultaneous system:

</div>

:::

:::

:::{.column width="50%"}

:::{.fragment}

<div style="font-size:16pt">

$$\left\{\frac{\partial L}{\partial \beta_j} = 0,~~\text{for}~~ j = 0, 1,\cdots, m\right.$$

</div>

:::

:::

::::

<div style="font-size:18pt">

:::{.fragment}

Depending on the form of the model $f\left(x\right)$ the equations in the system above may be non-linear and difficult to solve. 

:::

:::{.fragment}

It is common to choose $f\left(x\right)$ to be a *linear combination* of *base functions* $f_i\left(x\right)$ so that

$$f\left(x\right) = \beta_0f_0\left(x\right) + \beta_1f_1\left(x\right) + \cdots + \beta_mf_m\left(x\right)$$

:::

:::{.fragment}

Doing this forces the simultaneous system to be linear in the $\beta_i$ values. 

:::

:::{.fragment}

As an example, if the fitted function is to be a polynomial, then we have $f_0\left(x\right) = 1$, $f_1\left(x\right) = x$, $f_2\left(x\right) = x^2$, and so on. Resulting in

:::

:::{.fragment}

$$f\left(x\right) = \beta_0 + \beta_1x + \beta_2x^2 + \cdots + \beta_mx^m$$

:::

</div>

## Measuring Error

:::{.fragment}

Because we seek to capture a general trend, we know that the fitted model will not pass through all of the observed data points. 

:::

:::{.fragment}

We can define the standard error of the model residuals (prediction errors) to be

$$s_E = \sqrt{\left(\frac{L}{n - m}\right)}$$

where $L$ denotes the loss of the fitted function over all the observed data.

:::

:::{.fragment}

If $n+1 = m+1$ (if there is a parameter for every observation) then the model is an interpolant and the $s_E$ is undefined since it takes a "zero over zero" form.

:::

## Fitting a Straight-Line Model

:::{.fragment data-fragment-index=1}

<div style="font-size:17pt">

A simple linear regression model is a model of the form $f\left(x\right) = \beta_0 + \beta_1x$ which is fit to observed data of the form $\left(x_i, y_i\right)$ by minimizing the sum of squared residuals. 

</div>

:::

::::{.columns}

:::{.column width="50%"}

:::{.fragment data-fragment-index=2}

<div style = "font-size:17pt">

In this case we can analyze our Loss function as follows:

</div>

:::

:::{.fragment data-fragment-index=3}

<div style="font-size:16pt">

\begin{align*} L\left(\beta_0, \beta_1\right) &= \sum_{i = 0}^{n}{\left[y_i - f\left(x_i\right)\right]^2}\\
&= \sum_{i = 0}^{n}{\left[y_i - \beta_0 - \beta_1x_i\right]^2}
\end{align*}

</div>

:::

:::{.fragment data-fragment-index=4}

<div style="font-size:17pt">

We can minimize $L\left(\beta_0, \beta_1\right)$ by solving the system:

</div>

<div style="font-size:16pt">

$$\left\{\begin{array}{lcl} \frac{\partial L}{\partial \beta_0} & = & 0\\
\frac{\partial L}{\partial \beta_1} & = & 0\end{array}\right.$$

</div>

:::

:::{.fragment data-fragment-index=6}

<div style="font-size:17pt">

The top equation in the last line to the right indicates that $\beta_0 = \bar{y} - \beta_1\bar{x}$. 

</div>

:::


:::

:::{.column width="50%"}

:::{.fragment data-fragment-index=5}

<div style="font-size:18pt">

$$\implies \left\{\begin{array}{lcl} \sum{-2\left(y_i - \beta_0 - \beta_1x_i\right)} & = & 0\\
\sum{-2x_i\left(y_i - \beta_0 - \beta_1x_i\right)} & = & 0\end{array}\right.$$
$$\implies \left\{\begin{array}{lcl} \sum{\left(y_i - \beta_0 - \beta_1x_i\right)} & = & 0\\
\sum{\left(x_iy_i - \beta_0x_i - \beta_1x_i^2\right)} & = & 0\end{array}\right.$$
$$\implies \left\{\begin{array}{lcl} \sum{\left(\frac{y_i}{n+1} - \frac{\beta_0}{n+1} - \frac{\beta_1x_i}{n+1}\right)} & = & 0\\
\sum{\left(\frac{x_iy_i}{n+1} - \frac{\beta_0x_i}{n+1} - \frac{\beta_1x_i^2}{n+1}\right)} & = & 0\end{array}\right.$$
$$\implies \left\{\begin{array}{lcl} \bar{y} - \beta_0 - \beta_1\bar{x} & = & 0\\
- \beta_0\bar{x} + \sum{\left(\frac{x_iy_i}{n+1} - \frac{\beta_1x_i^2}{n+1}\right)} & = & 0\end{array}\right.$$

</div>

:::

:::

::::

<div style="font-size:17pt">

:::{.fragment data-fragment-index=7}

We can substitute this into the bottom equation and use some algebra to arrive at $\displaystyle{\beta_1 = \frac{\sum{y_i\left(x_i - \bar{x}\right)}}{\sum{x_i\left(x_i - \bar{x}\right)}}}$. ($\bigstar$ -- the algebra required is included at the end of this slide deck for those interested)

:::

</div>

## Fitting General Linear Forms

:::{.fragment}

<div style="font-size:18pt">

Consider the least-squares fit

</div>

<div style="font-size:16pt">

\begin{align*}f\left(x\right) &= \beta_0f_0\left(x\right) + \beta_1f_1\left(x\right) + \beta_2f_2\left(x\right) + \cdots + \beta_mf_m\left(x\right)\\
&= \sum_{j=0}^{m}{\beta_j f_j\left(x\right)}
\end{align*}

</div>

:::

:::{.fragment}

<div style="font-size:18pt">

Substituting this into our *least squares loss function* gives

</div>

<div style="font-size:16pt">

$$L\left(\beta_0, \beta_1, \cdots, \beta_m\right) = \sum_{i=0}^{n}\left[y_i - \sum_{j=0}^{m}{\beta_j f_j\left(x_i\right)}\right]^2$$

</div>

:::

:::{.fragment}

<div style="font-size:18pt">

Which is minimized by the solution to the following linear system:

</div>

<div style="font-size:16pt">

$$\left\{\frac{\partial L}{\partial \beta_k} = -2\left(\sum_{i=0}^{n}{\left(\left(y_i - \sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)\right)}\right)f_k\left(x_i\right)\right)}\right) = 0\right.~~\text{for}~~k = 0, 1, \cdots, m$$

</div>

:::

## Minimizing the Loss

:::{.fragment}

<div style="font-size:18pt">

As a reminder, we are minimizing

</div>

<div style="font-size:16pt">

$$\left\{\frac{\partial L}{\partial \beta_k} = -2\left(\sum_{i=0}^{n}{\left(\left(y_i - \sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)\right)}\right)f_k\left(x_i\right)\right)}\right) = 0\right.~~\text{for}~~k = 0, 1, \cdots, m$$

</div>

:::

:::::{.fragment}

<div style="font-size:18pt">

We can divide both sides of each equation in the system by $-2$ and rearrange the summations to arrive at:

</div>

::::{.columns}

:::{.column width="45%"}

<div style="font-size:16pt">

\begin{align*}\left\{\left(\sum_{i=0}^{n}{\left(\left(y_i - \sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)\right)}\right)f_k\left(x_i\right)\right)}\right)\right. &= 0\\
\implies \left\{\sum_{i=0}^{n}{\left(y_if_k\left(x_i\right) - \sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)f_k\left(x_i\right)\right)}\right)}\right. &= 0\\
\implies \left\{\sum_{i=0}^{n}{\left(y_if_k\left(x_i\right)\right)} - \sum_{i=0}^{n}{\left(\sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)f_k\left(x_i\right)\right)}\right)}\right. &= 0\\
\implies \left\{\sum_{i=0}^{n}{\left(\sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)f_k\left(x_i\right)\right)}\right)}\right. &= \sum_{i=0}^{n}{\left(y_if_k\left(x_i\right)\right)}
\end{align*}

</div>

:::

:::{.column width="5%"}

<div style="height:100%; border-right:3px solid #444;"></div>

:::

:::{.column width="5%"}

<div style="height:100%; border-left:3px solid #444;"></div>

:::

:::{.column width="45%"}

<div style="font-size:16pt">

\begin{align*} 
\implies \left\{\sum_{j=0}^{m}{\left(\sum_{i = 0}^{n}{\left(\beta_j f_j\left(x_i\right)f_k\left(x_i\right)\right)}\right)}\right. &= \sum_{i=0}^{n}{\left(y_if_k\left(x_i\right)\right)}\\
\implies \left\{\sum_{j=0}^{m}{\left(\sum_{i = 0}^{n}{\left(f_j\left(x_i\right)f_k\left(x_i\right)\right)\beta_j}\right)}\right. &= \sum_{i=0}^{n}{\left(f_k\left(x_i\right)y_i\right)}~~\text{for}~~k = 0, 1, \cdots, m\\
\end{align*}

</div>

:::

::::

:::::

## Minimizing the Loss

::::{.columns}

:::{.column width="45%"}

<div style="font-size:16pt">

\begin{align*}\left\{\left(\sum_{i=0}^{n}{\left(\left(y_i - \sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)\right)}\right)f_k\left(x_i\right)\right)}\right)\right. &= 0\\
\implies \left\{\sum_{i=0}^{n}{\left(y_if_k\left(x_i\right) - \sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)f_k\left(x_i\right)\right)}\right)}\right. &= 0\\
\implies \left\{\sum_{i=0}^{n}{\left(y_if_k\left(x_i\right)\right)} - \sum_{i=0}^{n}{\left(\sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)f_k\left(x_i\right)\right)}\right)}\right. &= 0\\
\implies \left\{\sum_{i=0}^{n}{\left(\sum_{j = 0}^{m}{\left(\beta_j f_j\left(x_i\right)f_k\left(x_i\right)\right)}\right)}\right. &= \sum_{i=0}^{n}{\left(y_if_k\left(x_i\right)\right)}
\end{align*}

</div>

:::

:::{.column width="5%"}

<div style="height:100%; border-right:3px solid #444;"></div>

:::

:::{.column width="5%"}

<div style="height:100%; border-left:3px solid #444;"></div>

:::

:::{.column width="45%"}

<div style="font-size:16pt">

\begin{align*} 
\implies \left\{\sum_{j=0}^{m}{\left(\sum_{i = 0}^{n}{\left(\beta_j f_j\left(x_i\right)f_k\left(x_i\right)\right)}\right)}\right. &= \sum_{i=0}^{n}{\left(y_if_k\left(x_i\right)\right)}\\
\implies \left\{\sum_{j=0}^{m}{\left(\sum_{i = 0}^{n}{\left(f_j\left(x_i\right)f_k\left(x_i\right)\right)\beta_j}\right)}\right. &= \sum_{i=0}^{n}{\left(f_k\left(x_i\right)y_i\right)}~~\text{for}~~k = 0, 1, \cdots, m\\
\end{align*}

</div>

:::

::::

:::{.fragment}

<div style="font-size:18pt">

We can rewrite the above using matrix notation as $A\vec{\beta} = \vec{b}$, where

</div>

<div style="font-size:16pt">

$$A_{kj} = \sum_{i = 0}^{n}{f_j\left(x_i\right)f_k\left(x_i\right)}~~~~\text{and}~~~~b_k = \sum_{i=0}^{n}{f_k\left(x_i\right)y_i}$$

</div>

:::

:::{.fragment}

<div style="font-size:18pt">

These equations are known as the *normal equations* of the least-squares fit, and can be solved using our numerical methods for solving [symmetric] linear systems!

</div>

:::

## Polynomial Fit

<div style="font-size:18pt">

We can rewrite the above using matrix notation as $A\vec{\beta} = \vec{b}$, where

</div>

<div style="font-size:16pt">

$$A_{kj} = \sum_{i = 0}^{n}{f_j\left(x_i\right)f_k\left(x_i\right)}~~~~\text{and}~~~~b_k = \sum_{i=0}^{n}{f_k\left(x_i\right)y_i}$$

</div>

<div style="font-size:18pt">

These equations are known as the *normal equations* of the least-squares fit, and can be solved using our numerical methods for solving [symmetric] linear systems!

</div>

<div style="font-size:18pt">

:::{.fragment}

As mentioned earlier, a commonly used linear form is a polynomial. 

:::

:::{.fragment}

In this case, the basis functions are $f_j\left(x\right) = x^j$. 

:::

:::{.fragment}

This leads to the following $A$ and $\vec{b}$ from the normal equations above:

$$A_{kj} = \sum_{i = 0}^{n}{x_i^{j+k}}~~~~\text{and}~~~~b_k = \sum_{i=0}^{n}{x_i^ky_i}$$

:::

:::{.fragment}

The coefficient matrix $A$ becomes increasingly ill-conditioned as $m$ is made larger. 

:::

:::{.fragment}

Luckily, high-degree polynomials are typically not used in curve-fitting since they are very susceptible to fitting the noise in the observed data.

:::

</div>

## Polynomial Least Squares Algorithm

<div style="font-size:18pt">

:::{.fragment}

With all of that math understood, we are ready to construct a routine to fit least-squares polynomial models. 

:::

:::{.fragment}

Since we need to solve symmetric linear systems as part of solving the normal equations, we'll need access to functionality from earlier in our course.

:::

:::{.fragment}

As a reminder, you make this older functionality available in a new notebook by pasting it into a code cell at the top of your notebook and running that code cell.

:::

:::{.fragment}

Because observed data can come in any order, we should be worried about when to *pivot* as we solve our linear system. For this reason, we'll bring in the `swapRows()`, `scaleFactors()`, and `gaussPivot()` functions from our Day 7 dicussion.

:::

:::{.fragment}

I've already enabled them in this slide deck, for convenience. I won't show the pasting here.

:::

</div>

## Polynomial Least Squares Algorithm

<div style="font-size:18pt">

::::{.columns}

:::{.column width="50%"}

:::{.fragment}

With those earlier functions available to us, we'll write our `polyFit()` routine. 

:::

:::{.fragment}

This routine will return the coefficients of a linear model of the form

$$f\left(x\right) = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_m x^m$$

fit on training data of the form $\left(x_i, y_i\right)$ with `xData` holding the $x_i$ values an `yData` holding the $y_i$ values. 

:::

:::{.fragment}

The function will take one additional parameter, `m`, which is the degree of the polynomial model.

:::

:::

:::{.column width="50%"}

:::{.fragment}

```{python}
#| echo: true
#| eval: true

def polyFit(xData, yData, m):
  A = np.zeros((m+1, m+1))
  b = np.zeros(m+1)
  s = np.zeros(2*m+1)

  for i in range(len(xData)):
    temp = yData[i]
    for j in range(m+1):
      b[j] = b[j] + temp
      temp = temp*xData[i]

    temp = 1.0

    for j in range(2*m + 1):
      s[j] = s[j] + temp
      temp = temp*xData[i]

  for i in range(m+1):
    for j in range(m+1):
      A[i, j] = s[i + j]

  return gaussPivot(A, b)

def evalPoly(coefs, x):
  m = len(coefs) - 1
  p = coefs[m]
  for j in range(m):
    p = p*x + coefs[m - j - 1]

  return p

def getStdError(xData, yData, coefs):
  n = len(xData)
  m = len(coefs)
  sse = 0.0
  for i in range(n):
    y_hat = evalPoly(coefs, xData[i])
    resid = yData[i] - y_hat
    sse = sse + resid**2

  stdError = (sse/(n - m))**0.5
  return stdError

def plotPoly(xData, yData, coefs, num_pts = 100, xlab = "x", ylab = "y"):
  m = len(coefs)
  x1 = min(xData)
  x2 = max(xData)
  x_new = np.linspace(x1, x2, num_pts)
  y_new = np.zeros(len(x_new))

  for i in range(num_pts):
    y_new[i] = evalPoly(coefs, x_new[i])

  plt.scatter(xData, yData, color = "black", s = 150)
  plt.plot(x_new, y_new, color = "purple", linewidth = 3)
  plt.grid()
  plt.axhline(color = "black")
  plt.axvline(color = "black")
  plt.xlabel(xlab)
  plt.ylabel(ylab)
  plt.title("A Polynomial Least Squares Fit of Degree " + str(m - 1), fontsize = 20)
  plt.show()
```

:::

:::

::::

</div>

## Comments on the Functionality

:::{.fragment}

The previous slide gave your four new functions that are useful for least squares regression (curve fitting) applications. You'll directly use all of them.

:::

+ The `polyFit()` function takes in the observed data and the desired degree of the polynomial model and returns the fitted coefficients as solutions to the normal equations.
+ The `evalPoly()` function allows you to evaluate your fitted polynomial for new observations. That is, this is the function that allows you to make predictions.
+ The `getStdError()` function computes the standard error of residuals. This is a measure of model performance. Smaller indicates a better fit, but you should be guided by held out test data here rather than reusing your training data. Take MAT300 to find out why.
+ The `plotPoly()` function is a convenience function that will handle plotting your training data and fitted polynomial model for you.

## Example Usage

:::::{.fragment}

::::{.columns}

:::{.column width="70%"}

**Example:** Fit a second-order (quadratic) linear regression model to the observed data on the right.

:::

:::{.column width="30%"}

$x$ | $y$ 
----|----
4 | 3.33
10 | 47.64
0 | 11.76
9 | 33.51
6 | 9.54
5 | 2.19

:::

::::

:::::

:::::{.fragment}

::::{.columns}

:::{.column width="50%"}

```{python}
#| echo: true
#| eval: false

xData = np.array([4.0, 10, 0, 9, 6, 5])
yData = np.array([3.33, 47.64, 11.76, 33.51, 9.54, 2.19])

coefs = polyFit(xData, yData, 2)
se = getStdError(xData, yData, coefs)

print("The model standard error is: ", se)
plotPoly(xData, yData, coefs)
```

:::

:::{.column width="50%"}

```{python}
#| echo: false
#| eval: true

xData = np.array([4.0, 10, 0, 9, 6, 5])
yData = np.array([3.33, 47.64, 11.76, 33.51, 9.54, 2.19])

coefs = polyFit(xData, yData, 2)
se = getStdError(xData, yData, coefs)

print("The model standard error is: ", se)
plotPoly(xData, yData, coefs)
```

:::

::::

:::::

## Summary

+ In this discussion we saw how to fit a curve to data using the method of *Ordinary Least Squares*. 
+ In particular, we worked with polynomial models, but we developed the general form of the *normal equations* for least squares which apply for any model which is a linear combination of *basis functions* $f_i\left(x\right)$. 
+ Finally, we wrote a routine that will fit a polynomial model of desired degree to observed data and included some helper functions to evaluate and plot the polynomial.

## Next Time

<br/>
<br/>

We'll pause here to encounter our first *Unit Problem Set*. This means that you'll spend the next week working through several interesting applications which can be addressed using one or more of the methods we've encountered so far in our course.

## Owed Algebra

<div style="font-size:16pt">

Earlier in this notebook it was suggested that, after a bit of algebra, we could obtain $\displaystyle{\beta_1 = \frac{\sum{y_i\left(x_i - \bar{x}\right)}}{\sum{x_i\left(x_i - \bar{x}\right)}}}$. The algebra required for that appears below. Recall that we have $\beta_0 = \bar{y} - \beta_1\bar{x}$. From here we have

</div>

<div style="font-size:14pt">
\begin{align*} -\beta_0\bar{x} + \sum{\left(\frac{x_iy_i}{n+1} - \frac{\beta_1x_i^2}{n+1}\right)} &= 0\\
\implies -\left(\bar{y} - \beta_1 \bar{x}\right)\bar{x} + \sum{\left(\frac{x_iy_i}{n+1} - \frac{\beta_1x_i^2}{n+1}\right)} &= 0\\
\implies -\bar{x}\bar{y} + \beta_1\bar{x}^2 + \sum{\frac{x_iy_i}{n+1}} - \beta_1\sum{\frac{x_i^2}{n+1}} &= 0\\
\implies \beta_1\sum{\frac{x_i^2}{n+1}} - \beta_1\bar{x}^2  &= \sum{\frac{x_iy_i}{n+1}} - \bar{x}\bar{y} \\
\implies \beta_1\left(\sum{\frac{x_i^2}{n+1}} - \left(\sum{\frac{x_i}{n+1}}\right)^2\right) &= \sum{\frac{x_iy_i}{n+1}} - \left(\sum{\frac{x_i}{n+1}}\right)\left(\sum{\frac{y_i}{n+1}}\right)\\
\implies \beta_1\left(\sum{\frac{x_i^2}{n+1}} - \left(\sum{\frac{x_i}{n+1}}\right)\left(\sum{\frac{x_i}{n+1}}\right)\right) &= \sum{\frac{x_iy_i}{n+1}} - \left(\sum{\frac{x_i}{n+1}}\right)\left(\sum{\frac{y_i}{n+1}}\right)\\
\implies \beta_1\left(\sum{\left(\frac{x_i}{n+1}\left(x_i - \sum{\frac{x_i}{n+1}}\right)\right)}\right) &= \sum{\left(\frac{y_i}{n+1}\left(x_i - \sum{\frac{x_i}{n+1}}\right)\right)}\\
\implies \beta_1\sum{\left(\frac{x_i}{n+1}\left(x_i - \bar{x}\right)\right)} &= \sum{\left(\frac{y_i}{n+1}\left(x_i - \bar{x}\right)\right)}\\
\implies \beta_1\sum{x_i\left(x_i - \bar{x}\right)} &= \sum{y_i\left(x_i - \bar{x}\right)}\\
\implies \beta_1 &= \frac{\sum{y_i\left(x_i - \bar{x}\right)}}{\sum{x_i\left(x_i - \bar{x}\right)}}~~\checkmark
\end{align*}

</div>