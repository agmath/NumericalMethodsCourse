{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgKRSZ5yiYbnxjvHdrPS4O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"IaUuXB9J0j8w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question:** Look forward to determine whether we need `LUdecomp5solver()`. If that routine is not used later, we can remove half of this notebook."],"metadata":{"id":"Z7cUrynYyPQm"}},{"cell_type":"markdown","source":["# Day 6: Symmetric and Banded Coefficient Matrices\n","\n","Applications often involve coefficient matrices that are *sparse*ly populated. That means most of their entries are $0$'s. If all of the non-zero entries are clustered near the main diagonal, then the matrix is said to be *banded*. For example, see the matrix below, which is a tri-diagonal, banded matrix.\n","\n","$$\\left[\\begin{array}{ccccccc}X & X & 0 & 0 & \\cdots & 0 & 0 & 0\\\\\n","X & X & X & 0 & \\cdots & 0 & 0 & 0\\\\\n","0 & X & X & X & \\cdots & 0 & 0 & 0\\\\[10pt]\n","\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots & \\vdots\\\\[10pt]\n","0 & 0 & 0 & 0 &\\cdots & X & X & X\\\\\n","0 & 0 & 0 & 0 & \\cdots & 0 & X & X\n","\\end{array}\\right]$$\n","\n","Those $X$ entries could be zero or non-zero, but all entries off of those three diagonals are $0$'s in a tri-diagonal matrix. We say that such a matrix has a *bandwidth* of $3$ since there are at most three non-zero elements in each row.\n","\n","A nice property of these banded matrices is that, if they are *LU-decomposed*, then both $L$ and $U$ retain the banded structure. This structure can be exploited to save on both *memory requirements* and *run time*."],"metadata":{"id":"tsbX-Cv8X2So"}},{"cell_type":"markdown","source":["## An Example\n","\n","To motivate our discovery and implementation of this method, let's try solving a system with a tridiagonal coefficient matrix.\n","\n","**Example:** Solve the system\n","$$A = \\left[\\begin{array}{ccccc} 1 & 2 & 0 & 0 & 0\\\\\n","2 & -1 & 8 & 0 & 0\\\\\n","0 & 3 & -1 & -1 & 0\\\\\n","0 & 0 & 3 & 2 & -1\\\\\n","0 & 0 & 0 & 5 & -4\\end{array}\\right]\\left[\\begin{array}{c} x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\end{array}\\right] = \\left[\\begin{array}{c}13\\\\ 30\\\\ 7\\\\ 12\\\\ 6\\end{array}\\right]$$\n","\n","> *Solution.*"],"metadata":{"id":"oo0ThT6ztAzG"}},{"cell_type":"markdown","source":["### Doolittle's Method for Tridiagonal Coefficient Matrices\n","\n","Consider the system $A\\vec{x} = \\vec{b}$, where $A$ is a tridiagonal matrix of the following form:\n","\n","$$A = \\left[\\begin{array}{cccccc} d_1 & e_1 & 0 & 0 & \\cdots & 0\\\\\n","c_1 & d_2 & e_2 & 0 & \\cdots & 0\\\\\n","0 & c_2 & d_3 & e_3 & \\cdots & 0\\\\\n","0 & 0 & c_3 & d_4 & \\cdots & 0\\\\\n","\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & \\cdots & 0 & c_{n-1} & d_n\n","\\end{array}\\right]$$\n","\n","In this case, the majority of the entries will be $0$ (as long as the matrix is large enough). It becomes more efficient to store the three diagonal vectors only, since we know that all other entries are $0$'s. That is, instead of storing $A$ explicitly, we'll store:\n","\n","$$\\vec{c} = \\left[\\begin{array}{c} c_1\\\\ c_2\\\\ \\vdots\\\\ c_{n-1}\\end{array}\\right]~~~\\vec{d} = \\left[\\begin{array}{c} d_1\\\\ d_2\\\\ \\vdots\\\\ d_n\\end{array}\\right]~~~\\vec{e} = \\left[\\begin{array}{c} e_1\\\\ e_2\\\\ \\vdots\\\\ e_{n-1}\\end{array}\\right]$$\n","\n","This is a significant savings, since a $100\\times 100$ tridiagonal matrix has $10,000$ entries, but we can get away with storing only $99 + 100 + 99 = 298$ entries using this vector storage trick. This is a *compression* of around $33:1$."],"metadata":{"id":"baGo5ecYaiej"}},{"cell_type":"markdown","source":["We can still apply *LU decomposition* to the coefficient \"matrix\" (really, the coefficient vectors) as long as we carefully track which vectors we are working with. As a reminder, the *LU-decomposition* begins with the usual *Gaussian Elimination* procedure. That is, to reduce $R_k$ (row $k$) we eliminate $c_{k-1}$ using:\n","\n","$$R_k \\leftarrow R_k - \\left(c_{k-1}/d_{k-1}\\right)R_{k-1}$$\n","\n","Notice that in doing this, only $\\vec{c}$ and $\\vec{d}$ are changed. The changes are:\n","\n","$$\\begin{array}{lcl} d_k &\\leftarrow &d_k - \\left(c_{k-1}/d_{k-1}\\right)e_{k-1}\\\\\n","c_{k-1} &\\leftarrow & 0\n","\\end{array}$$\n","\n","At the end of the process, the vector $\\vec{c}$ would be zeroed out. It is a waste of space to store that zero-vector, but $\\vec{c}$ is a perfect place to store our $\\lambda$ scalar multipliers for our pivot rows. For this reason, we'll update:\n","\n","$$\\begin{array}{lcl} d_k &\\leftarrow &d_k - \\left(c_{k-1}/d_{k-1}\\right)e_{k-1}\\\\\n","c_{k-1} &\\leftarrow & c_{k-1}/d_{k-1}\n","\\end{array}$$\n","\n","Thus, we have the following decomposition algorithm:\n","\n","```\n","#tridiagonal LU decomp\n","for k in range(1, n):\n","  lam = c[k-1]/d[k-1]\n","  d[k] = d[k] - lam*e[k-1]\n","  c[k-1] = lam\n","```\n","\n","Now we'll look to the *solution phase*. As a reminder, we're solving $A\\vec{x} = \\vec{b}$ by solving $LU\\vec{x} = \\vec{b}$. The strategy is to use forward-substitution to solve $L\\vec{y} = \\vec{b}$ and then backward-substitution to solve $U\\vec{x} = \\vec{y}$ as before, but we need to deal with the fact that our factorized matrix elements are split across the three vectors $\\vec{c}$, $\\vec{d}$, and $\\vec{e}$.\n","\n","We can solve $L\\vec{y} = \\vec{b}$ as follows (remember that $\\vec{c}$ contains our $\\lambda$ multipliers):\n","\n","```\n","#forward substitution\n","y[0] = b[0]\n","for k in range(1, n):\n","  y[k] = b[k] - c[k-1]*y[k-1]\n","```\n","\n","Now we solve $U\\vec{x} = \\vec{y}$ using backward-substitution.\n","\n","```\n","#backward substitution\n","x[n-1] = y[n-1]/d[n-1]\n","for k in range(n-2, -1, -1):\n","  x[k] = (y[k] - e[k]*x[k+1])/d[k]\n","```\n","\n","We'll put this all together into a `DoolittleLUdecomp3solver()` routine below. The routine should make use of two *helper functions*, `DoolittleLUdecomp3()` and `Doolittle3solver()`, which are described below.\n","\n","+ `DoolittleLUdecomp3()` should take three parameters -- arrays `c`, `d`, and `e` representing the diagonals of the tridiagonal coefficient matrix as described above, and should return the transformed versions of `c`, `d`, and `e` resulting from the row reduction operations.\n","+ `Doolittle3solver()` should take four parameters -- a scalar `lam` and the vectors `c`, `d`, and `e` resulting from the `DoolittleLUdecomp3()` function. This function should return the solution vector for $A\\vec{x} = \\vec{b}$.\n","+ `DoolittleLUdecomp3solver()` should take four parameters -- arrays `c`, `d`, and `e` representing the diagonals of the tridiagonal coefficient matrix and the constant array `b`. The function should make use of `DoolittleLUdecomp3()` and `Doolittle3solver()` to return the solution of the matrix equation $A\\vec{x} = \\vec{b}$, where $A$ is a tridiagonal matrix with lower diagonal `c`, main diagonal `d`, and upper diagonal `e`."],"metadata":{"id":"ahrptGGvfd7C"}},{"cell_type":"code","source":["#Define your function and helper functions here.\n"],"metadata":{"id":"Tz9LAL_Vq24u","executionInfo":{"status":"ok","timestamp":1689428729530,"user_tz":240,"elapsed":182,"user":{"displayName":"Adam Gilbert","userId":"02074648007699947871"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Verify that this solver works on our example from earlier!"],"metadata":{"id":"wkefoNlJ4i-1"}},{"cell_type":"code","source":["#Solve the example problem here\n"],"metadata":{"id":"vQ-6jn3ws62U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Symmetric Coefficient Matrices\n","\n","Because of how often applications result in symmetric, banded coefficient matrices (where $\\vec{c} = \\vec{e}$), mathematicians and engineers spend time developing specialized approaches that work more quickly and efficiently than general approaches. For example, it can be shown that, if a matrix $A$ is symmetric, then its *LU-decomposition* can be written as\n","\n","$$A = LU = LDL^T$$\n","\n","where $D$ is a *diagonal matrix*. This means that we can still use Doolittle's decomposition with $U = DL^T$. That is,\n","\n","\\begin{align*} U &= DL^T\\\\\n","&= \\left[\\begin{array}{ccccc} D_1 & 0 & 0 & \\cdots & 0\\\\\n","0 & D_2 & 0 & \\cdots & 0\\\\\n","0 & 0 & D_3 & \\cdots & 0\\\\\n","\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & 0 & \\cdots & D_n\n","\\end{array}\\right]\\left[\\begin{array}{ccccc} 1 & L_{21} & L_{31} & \\cdots & L_{n1}\\\\\n","0 & 1 & L_{32} & \\cdots & L_{n2}\\\\\n","0 & 0 & 1 & \\cdots & L_{n3}\\\\\n","\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & 0 & \\cdots & 1\n","\\end{array}\\right]\\\\\n","&= \\left[\\begin{array}{ccccc} D_1 & D_1L_{21} & D_1L_{31} & \\cdots & D_1L_{n1}\\\\\n","0 & D_2 & D_2L_{32} & \\cdots & D_2L_{n2}\\\\\n","0 & 0 & D_3 & \\cdots & D_3L_{n3}\\\\\n","\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & 0 & \\cdots & D_n\n","\\end{array}\\right]\n","\\end{align*}\n","\n","Again, as a space-saving efficiency, we can choose to only store $U$ since both $L$ and $D$ are recoverable from this matrix! Again, the *Gaussian Elimination* procedure which results in an upper triangular matrix is sufficient to decompose a symmetric matrix.\n","\n","There is an alternative, however, which is even more efficient due to speed-ups at the solution phase. We can store\n","\n","$$U^* = \\left[\\begin{array}{ccccc} D_1 & L_{21} & L_{31} & \\cdots & L_{n1}\\\\\n","0 & D_2 & L_{32} & \\cdots & L_{n2}\\\\\n","0 & 0 & D_3 & \\cdots & L_{n3}\\\\\n","\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & 0 & \\cdots & D_n\n","\\end{array}\\right]$$\n","\n","and notice that $U_{ij} = D_iL_{ji} = U^*_{ii}*U^*_{ij}$. Since this is the case, we'll choose to store and utilize $U^*$ through our numerical routine."],"metadata":{"id":"b33XLGxg4mh9"}},{"cell_type":"markdown","source":["***"],"metadata":{"id":"dTmH5BKFzRbG"}},{"cell_type":"markdown","source":["## Optional: Symmetric, Pentadiagonal Coefficient Matrices\n","\n","We often encounter bandwidth $5$ coefficient matrices when we attempt numerical approaches to solving fourth-order ODEs by finite difference methods. These matrices are of the form\n","\n","$$A = \\left[\\begin{array}{cccccccc} d_1 & e_1 & f_1 & 0 & 0 & 0 & \\cdots & 0\\\\\n","e_1 & d_2 & e_2 & f_2 & 0 & 0 & \\cdots & 0\\\\\n","f_1 & e_2 & d_3 & e_3 & f_3 & 0 & \\cdots & 0\\\\\n","0 & f_2 & e_3 & d_4 & e_4 & f_4 & \\cdots & 0\\\\\n","\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & \\cdots & 0 & f_{n-4} & e_{n-3} & d_{n-3} & d_{n-2} & f_{n-2}\\\\\n","0 & \\cdots & 0 & 0 & f_{n-3} & e_{n-2} & d_{n-1} & e_{n-1}\\\\\n","0 & \\cdots & 0 & 0 & 0 & f_{n-2} & e_{n-1} & d_n\n","\\end{array}\\right]$$\n","\n","As in the case of our tridiagonal matrices, we'll store the non-zero elements in three vectors:\n","\n","$$\\vec{d} = \\left[\\begin{array}{c} d_1\\\\ d_2\\\\ \\vdots\\\\ d_{n-2}\\\\ d_{n-1}\\\\ d_{n}\\end{array}\\right]~~~~ \\vec{e} = \\left[\\begin{array}{c} e_1\\\\ e_2\\\\ \\vdots\\\\ e_{n-2}\\\\ e_{n-1}\\end{array}\\right]~~~~ \\vec{f} = \\left[\\begin{array}{c} f_1\\\\ f_2\\\\ \\vdots\\\\ f_{n-2}\\end{array}\\right]$$\n","\n","We'll again use Doolittle's decomposition to transform $A$ to upper triangular form by *Gaussian Elimination*. Consider the stage where the $k^{th}$ row has become the pivot row. Then, we have\n","\n","$$\\left[\\begin{array}{cc|ccc|cccc} \\ddots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\\\ \\hline\n","\\cdots & 0 & d_k & e_k & f_k & 0 & 0 & 0 & \\cdots \\\\\n","\\cdots & 0 & e_k & d_{k+1} & e_{k+1} & f_{k+1} & 0 & 0 & \\cdots \\\\\n","\\cdots & 0 & f_k & e_{k+1} & d_{k+2} & e_{k+2} & f_{k+2} & 0 & \\cdots\\\\ \\hline\n","\\cdots & 0 & 0 & f_{k+1} & e_{k+2} & d_{k+3} & e_{k+3} & f_{k+3} & \\cdots\\\\\n"," & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n","\\end{array}\\right]$$\n","\n","We eliminate $e_k$ in $R_{k+1}$ using\n","\n","$$R_{k+1} \\leftarrow R_{k+1} - \\left(e_k/d_k\\right)R_k$$\n","\n","Similarly we eliminate $f_k$ in $R_{k+2}$ using\n","\n","$$R_{k+2} \\leftarrow R_{k+2} - \\left(f_k/d_k\\right)R_k$$\n","\n","Other than the entries being zeroed out, the only terms being changed by this process are:\n","\n","\\begin{align*} d_{k+1} &\\leftarrow d_{k+1} - \\left(e_k/d_k\\right)e_k\\\\\n","e_{k+1} &\\leftarrow e_{k+1} - \\left(e_k/d_k\\right)f_k\\\\\n","d_{k+2} &\\leftarrow d_{k+2} - \\left(f_k/d_k\\right)f_k\n","\\end{align*}\n","\n","Recalling that we are constructing $U^*$, which stores the $\\lambda$ multipliers above the main diagonal, we can rewrite $e_k$ and $f_k$ with those corresponding multipliers. That is,\n","\n","\\begin{align*} e_k &\\leftarrow e_k/d_k\\\\\n","f_k &\\leftarrow f_k/d_k\n","\\end{align*}\n","\n","Once this is done, we'll have constructed a matrix of the following form (note $\\vec{d}$, $\\vec{e}$, and $\\vec{f}$ are not the original vectors we began with).\n","\n","$$U^* = \\left[\\begin{array}{cccccc} d_1 & e_1 & f_1 & 0 & \\cdots & 0\\\\\n","0 & d_2 & e_2 & f_2 & \\cdots & 0\\\\\n","0 & 0 & d_3 & e_3 & \\cdots & 0\\\\\n","\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & \\cdots & 0 & d_{n-1} & e_{n-1}\\\\\n","0 & 0 & \\cdots & 0 & 0 & d_n\n","\\end{array}\\right]$$\n","\n","We now enter the solution phase, where we again solve $LU\\vec{x} = \\vec{b}$ by first solving $L\\vec{y} = \\vec{b}$ and then $U\\vec{x} = \\vec{y}$. Note that $L\\vec{y} = \\vec{b}$ has the form below.\n","\n","$$\\left[L\\mid \\vec{b}\\right] = \\left[\\begin{array}{cccccc|c} 1 & 0 & 0 & 0 & \\cdots & 0 & b_1\\\\\n","e_1 & 1 & 0 & 0 & \\cdots & 0 & b_2\\\\\n","f_1 & e_2 & 1 & 0 & \\cdots & 0 & b_3\\\\\n","0 & f_2 & e_3 & 1 & \\cdots & 0 & b_4\\\\\n","\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n","0 & 0 & \\cdots & f_{n-2} & e_{n-1} & 1 & b_n\\\\\n","\\end{array}\\right]$$\n","\n","Which can be solved using forward substitution, as below:\n","\n","```\n","#forward substitution\n","#b[0] = b[0] #unnecessary\n","b[1] = b[1] - e[0]*b[0]\n","for k in range(2, n):\n","  b[k] = b[k] - (e[k-1]*b[k-1]) - (f[k-2]*b[k-2])\n","```\n","\n","The augmented coefficient matrix corresponding to $U\\vec{x} = \\vec{y}$ has the form below.\n","\n","$$\\left[U\\mid \\vec{y}\\right] = \\left[\\begin{array}{cccccc|c} d_1 & d_1e_1 & d_1f_1 & 0 & \\cdots & 0 & y_1\\\\\n","0 & d_2 & d_2e_2 & d_2f_2 & \\cdots & 0 & y_2\\\\\n","0 & 0 & d_3 & d_3e_3 & \\cdots & 0 & y_3\\\\\n","\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n","0 & 0 & \\cdots & 0 & d_{n-1} & d_{n-1}e_{n-1} & y_{n-1}\\\\\n","0 & 0 & \\cdots & 0 & 0 & d_n & y_n\\\\\n","\\end{array}\\right]$$\n","\n","Again, this can be solved using back substitution as follows:\n","\n","```\n","b[n-1] = b[n-1]/d[n-1]\n","b[n-2] = (b[n-2]/d[n-2]) - (e[n-2]*b[n-1])\n","\n","for k in range(n-3, -1, -1):\n","  b[k] = b[k]/d[k] - (e[k]*b[k+1]) - (f[k]*b[k+2])\n","```\n","\n","Finally, we can define the `DoolittleLUdecomp5solver()` below!\n"],"metadata":{"id":"DgJutT9mPHZK"}},{"cell_type":"code","source":["def DoolittleLUdecomp5(d, e, f):\n","  n = len(d)\n","  for k in range(n - 2):\n","    lam = e[k]/d[k]\n","    d[k+1] = d[k+1] - lam*e[k]\n","    e[k+1] = e[k+1] - lam*f[k]\n","    e[k] = lam\n","    lam = f[k]/d[k]\n","    d[k+2] = d[k+2] - lam*f[k]\n","    f[k] = lam\n","\n","  lam = e[n-2]/d[n-2]\n","  d[n-1] = d[n-1] - lam*e[n-2]\n","  e[n-2] = lam\n","\n","  return d, e, f\n","\n","def Doolittle5solver(d, lam_e, lam_f, b):\n","  n = len(d)\n","  b[1] = b[1] - (lam_e[0]*b[0])\n","  for k in range(2, n):\n","    b[k] = b[k] - (lam_e[k-1]*b[k-1]) - (lam_f[k-2]*b[k-2])\n","\n","  b[n-1] = b[n-1]/d[n-1]\n","  b[n-2] = b[n-2]/d[n-2] - (lam_e[n-2]*b[n-1])\n","  for k in range(n-3, -1, -1):\n","    b[k] = b[k]/d[k] - (lam_e[k]*b[k+1]) - (lam_f[k]*b[k+2])\n","\n","  return b\n","\n","def DoolittleLUdecomp5solver(d, e, f, b):\n","  d, lam_e, lam_f = DoolittleLUdecomp5(d, e, f)\n","  x = Doolittle5solver(d, lam_e, lam_f, b)\n","\n","  return x"],"metadata":{"id":"YhEJXXlsepqC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll return to this `DoolittleLUdecomp5solver()` function later in our course. For now, we can use it to solve the following system (just to make sure our routine works!).\n","\n","**Example:** Use the routine we wrote to solve the following symmetric, pentadiagonal linear system $$\\left[\\begin{array}{ccccccc} 2 & -3 & 1 & 0 & 0 & 0 & 0\\\\\n","-3 & 1 & 4 & -2 & 0 & 0 & 0\\\\\n","1 & 4 & -6 & -1 & 1 & 0 & 0\\\\\n","0 & -2 & -1 & 5 & 4 & 2 & 0\\\\\n","0 & 0 & 1 & 4 & 3 & 5 & -3\\\\\n","0 & 0 & 0 & 2 & 5 & 2 & 1\\\\\n","0 & 0 & 0 & 0 & -3 & 1 & 4\n","\\end{array}\\right]\\left[\\begin{array}{c} x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\\\ x_6\\\\ x_7\\end{array}\\right] = \\left[\\begin{array}{c} -5\\\\ 3\\\\ 2\\\\ -11\\\\ 4\\\\ 3\\\\ 1\\end{array}\\right]$$"],"metadata":{"id":"evtyxbl9hC4f"}},{"cell_type":"code","source":["#Solve the example problem here\n"],"metadata":{"id":"XLvsyxWhhCK7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***\n","\n","## Summary\n","\n","In this notebook, we considered additional efficiencies that can be gained by exploiting properties of the coefficient matrix. In particular, we looked at two special cases of banded matrices and banded, symmetric matrices. We could construct specialty solvers for other classes of coefficient matrix by analyzing the process involved in using the *Gaussian Elimination* approach to solving the corresponding system. Taking an algorithmic approach is much slower for a single problem, but pays off because we end up with a constructed routine that can \"quickly\" solve any problem fitting the assumptions of the algorithm we've constructed."],"metadata":{"id":"zYLVApyRkl1_"}},{"cell_type":"code","source":[],"metadata":{"id":"a3camZaJlZhg"},"execution_count":null,"outputs":[]}]}