{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXg86CaaiekS8LvVtXYWTW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"IaUuXB9J0j8w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Day 7: Symmetric and Banded Coefficient Matrices\n","\n","Applications often involve coefficient matrices that are *sparse*ly populated. That means most of their entries are $0$'s. If all of the non-zero entries are clustered near the main diagonal, then the matrix is said to be *banded*. For example, see the matrix below, which is a tri-diagonal, banded matrix.\n","\n","$$\\left[\\begin{array}{ccccccc}X & X & 0 & 0 & \\cdots & 0 & 0 & 0\\\\\n","X & X & X & 0 & \\cdots & 0 & 0 & 0\\\\\n","0 & X & X & X & \\cdots & 0 & 0 & 0\\\\[10pt]\n","\\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots & \\vdots\\\\[10pt]\n","0 & 0 & 0 & 0 &\\cdots & X & X & X\\\\\n","0 & 0 & 0 & 0 & \\cdots & 0 & X & X\n","\\end{array}\\right]$$\n","\n","Those $X$ entries could be zero or non-zero, but all entries off of those three diagonals are $0$'s in a tri-diagonal matrix. We say that such a matrix has a *bandwidth* of $3$ since there are at most three non-zero elements in each row.\n","\n","A nice property of these banded matrices is that, if they are *LU-decomposed*, then both $L$ and $U$ retain the banded structure. This structure can be exploited to save on both *memory requirements* and *run time*."],"metadata":{"id":"tsbX-Cv8X2So"}},{"cell_type":"markdown","source":["## An Example\n","\n","To motivate our discovery and implementation of this method, let's try solving a system with a tridiagonal coefficient matrix.\n","\n","**Example:** Solve the system\n","$$A = \\left[\\begin{array}{ccccc} 1 & 2 & 0 & 0 & 0\\\\\n","2 & -1 & 8 & 0 & 0\\\\\n","0 & 3 & -1 & -1 & 0\\\\\n","0 & 0 & 3 & 2 & -1\\\\\n","0 & 0 & 0 & 5 & -4\\end{array}\\right]\\left[\\begin{array}{c} x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\end{array}\\right] = \\left[\\begin{array}{c}13\\\\ 30\\\\ 7\\\\ 12\\\\ 6\\end{array}\\right]$$\n","\n","> *Solution.*"],"metadata":{"id":"oo0ThT6ztAzG"}},{"cell_type":"markdown","source":["### Doolittle's Method for Tridiagonal Coefficient Matrices\n","\n","Consider the system $A\\vec{x} = \\vec{b}$, where $A$ is a tridiagonal matrix of the following form:\n","\n","$$A = \\left[\\begin{array}{cccccc} d_1 & e_1 & 0 & 0 & \\cdots & 0\\\\\n","c_1 & d_2 & e_2 & 0 & \\cdots & 0\\\\\n","0 & c_2 & d_3 & e_3 & \\cdots & 0\\\\\n","0 & 0 & c_3 & d_4 & \\cdots & 0\\\\\n","\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & \\cdots & 0 & c_{n-1} & d_n\n","\\end{array}\\right]$$\n","\n","In this case, the majority of the entries will be $0$ (as long as the matrix is large enough). It becomes more efficient to store the three diagonal vectors only, since we know that all other entries are $0$'s. That is, instead of storing $A$ explicitly, we'll store:\n","\n","$$\\vec{c} = \\left[\\begin{array}{c} c_1\\\\ c_2\\\\ \\vdots\\\\ c_{n-1}\\end{array}\\right]~~~\\vec{d} = \\left[\\begin{array}{c} d_1\\\\ d_2\\\\ \\vdots\\\\ d_n\\end{array}\\right]~~~\\vec{e} = \\left[\\begin{array}{c} e_1\\\\ e_2\\\\ \\vdots\\\\ e_{n-1}\\end{array}\\right]$$\n","\n","This is a significant savings, since a $100\\times 100$ tridiagonal matrix has $10,000$ entries, but we can get away with storing only $99 + 100 + 99 = 298$ entries using this vector storage trick. This is a *compression* of around $33:1$."],"metadata":{"id":"baGo5ecYaiej"}},{"cell_type":"markdown","source":["We can still apply *LU decomposition* to the coefficient \"matrix\" (really, the coefficient vectors) as long as we carefully track which vectors we are working with. As a reminder, the *LU-decomposition* begins with the usual *Gaussian Elimination* procedure. That is, to reduce $R_k$ (row $k$) we eliminate $c_{k-1}$ using:\n","\n","$$R_k \\leftarrow R_k - \\left(c_{k-1}/d_{k-1}\\right)R_{k-1}$$\n","\n","Notice that in doing this, only $\\vec{c}$ and $\\vec{d}$ are changed. The changes are:\n","\n","$$\\begin{array}{lcl} d_k &\\leftarrow &d_k - \\left(c_{k-1}/d_{k-1}\\right)e_{k-1}\\\\\n","c_{k-1} &\\leftarrow & 0\n","\\end{array}$$\n","\n","At the end of the process, the vector $\\vec{c}$ would be zeroed out. It is a waste of space to store that zero-vector, but $\\vec{c}$ is a perfect place to store our $\\lambda$ scalar multipliers for our pivot rows. For this reason, we'll update:\n","\n","$$\\begin{array}{lcl} d_k &\\leftarrow &d_k - \\left(c_{k-1}/d_{k-1}\\right)e_{k-1}\\\\\n","c_{k-1} &\\leftarrow & c_{k-1}/d_{k-1}\n","\\end{array}$$\n","\n","Thus, we have the following decomposition algorithm:\n","\n","```\n","#tridiagonal LU decomp\n","for k in range(___, ___):\n","  lam = ___[___]/___[___]\n","  ___[___] = ___[___] - ___*___[___]\n","  ___[___] = ___\n","```\n","\n","Now we'll look to the *solution phase*. As a reminder, we're solving $A\\vec{x} = \\vec{b}$ by solving $LU\\vec{x} = \\vec{b}$. The strategy is to use forward-substitution to solve $L\\vec{y} = \\vec{b}$ and then backward-substitution to solve $U\\vec{x} = \\vec{y}$ as before, but we need to deal with the fact that our factorized matrix elements are split across the three vectors $\\vec{c}$, $\\vec{d}$, and $\\vec{e}$.\n","\n","We can solve $L\\vec{y} = \\vec{b}$ as follows (remember that $\\vec{c}$ contains our $\\lambda$ multipliers):\n","\n","```\n","#forward substitution\n","y[0] = b[0]\n","for k in range(___, ___):\n","  #Can use b to store values of y, since the\n","  #original entris of b are no longer needed\n","  b[k] = b[k] - ___[___]*___[___]\n","```\n","\n","Now we solve $U\\vec{x} = \\vec{y}$ using backward-substitution.\n","\n","```\n","#backward substitution\n","x[n-1] = y[n-1]/d[n-1]\n","for k in range(n-2, -1, -1):\n","  #Similarly, can use b to store values of x\n","  b[k] = (b[k] - ___[___]*___[___])/___[___]\n","```\n","\n","We can put this all together into the `DoolittleLUdecomp3solver()` routine below."],"metadata":{"id":"ahrptGGvfd7C"}},{"cell_type":"code","source":["def DoolittleLUdecomp3(c, d, e):\n","  n = ___\n","  for k in range(___, ___):\n","    lam = ___[___]/___[___]\n","    ___[___] = ___[___] - ___*___[___]\n","    ___[___] = ___\n","\n","  return c, d, e\n","\n","def Doolittle3solver(lam, d, e, b):\n","  n = ___\n","  #Forward Substitution\n","  for k in range(___, ___):\n","    b[k] = b[k] - ___[___]*___[___]\n","\n","  b[n-1] = b[n-1]/d[n-1]\n","  for k in range(n-2, -1, -1):\n","    b[k] = (b[k] - ___[___]*___[___])/___[___]\n","\n","  return b\n","\n","def DoolittleLUdecomp3solver(c, d, e, b):\n","  lam, d, e = ___\n","  b = ___\n","\n","  return ___"],"metadata":{"id":"Tz9LAL_Vq24u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example:** Verify that this solver works on our example from earlier! As a reminder, our system corresponded to the augmented coefficient matrix\n","\n","$$A = \\left[\\begin{array}{ccccc} 1 & 2 & 0 & 0 & 0\\\\\n","2 & -1 & 8 & 0 & 0\\\\\n","0 & 3 & -1 & -1 & 0\\\\\n","0 & 0 & 3 & 2 & -1\\\\\n","0 & 0 & 0 & 5 & -4\\end{array}\\right]\\left[\\begin{array}{c} x_1\\\\ x_2\\\\ x_3\\\\ x_4\\\\ x_5\\end{array}\\right] = \\left[\\begin{array}{c}13\\\\ 30\\\\ 7\\\\ 12\\\\ 6\\end{array}\\right]$$"],"metadata":{"id":"wkefoNlJ4i-1"}},{"cell_type":"code","source":[],"metadata":{"id":"vQ-6jn3ws62U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Aside: Symmetric Coefficient Matrices and Increased Efficiency\n","\n","Because of how often applications result in symmetric, banded coefficient matrices (where $\\vec{c} = \\vec{e}$), mathematicians and engineers spend time developing specialized approaches that work more quickly and efficiently than general approaches. For example, it can be shown that, if a matrix $A$ is symmetric, then its *LU-decomposition* can be written as\n","\n","$$A = LU = LDL^T$$\n","\n","where $D$ is a *diagonal matrix*. This means that we can still use Doolittle's decomposition with $U = DL^T$. That is,\n","\n","\\begin{align*} U &= DL^T\\\\\n","&= \\left[\\begin{array}{ccccc} D_1 & 0 & 0 & \\cdots & 0\\\\\n","0 & D_2 & 0 & \\cdots & 0\\\\\n","0 & 0 & D_3 & \\cdots & 0\\\\\n","\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & 0 & \\cdots & D_n\n","\\end{array}\\right]\\left[\\begin{array}{ccccc} 1 & L_{21} & L_{31} & \\cdots & L_{n1}\\\\\n","0 & 1 & L_{32} & \\cdots & L_{n2}\\\\\n","0 & 0 & 1 & \\cdots & L_{n3}\\\\\n","\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & 0 & \\cdots & 1\n","\\end{array}\\right]\\\\\n","&= \\left[\\begin{array}{ccccc} D_1 & D_1L_{21} & D_1L_{31} & \\cdots & D_1L_{n1}\\\\\n","0 & D_2 & D_2L_{32} & \\cdots & D_2L_{n2}\\\\\n","0 & 0 & D_3 & \\cdots & D_3L_{n3}\\\\\n","\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & 0 & \\cdots & D_n\n","\\end{array}\\right]\n","\\end{align*}\n","\n","Again, as a space-saving efficiency, we can choose to only store $U$ since both $L$ and $D$ are recoverable from this matrix! Again, the *Gaussian Elimination* procedure which results in an upper triangular matrix is sufficient to decompose a symmetric matrix.\n","\n","There is an alternative, however, which is even more efficient due to speed-ups at the solution phase (since the $L_{ij}$ are not masked). We can instead store\n","\n","$$U^* = \\left[\\begin{array}{ccccc} D_1 & L_{21} & L_{31} & \\cdots & L_{n1}\\\\\n","0 & D_2 & L_{32} & \\cdots & L_{n2}\\\\\n","0 & 0 & D_3 & \\cdots & L_{n3}\\\\\n","\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n","0 & 0 & 0 & \\cdots & D_n\n","\\end{array}\\right]$$\n","\n","and notice that $U_{ij} = D_iL_{ji} = U^*_{ii}*U^*_{ij}$.\n","\n","We won't build an implementation to specifically handle symmetric matrices here because our `LUdecomp3solver()` routine will handle all of the instances for our course. I include this aside however to point out that there are researchers and scientist who spend their careers investigating, developing, and implementing ways to improve run-time-complexity and space-complexity of existing algorithms or developing brand new algorithms."],"metadata":{"id":"b33XLGxg4mh9"}},{"cell_type":"markdown","source":["***\n","\n","## Summary\n","\n","In this notebook, we considered additional efficiencies that can be gained by exploiting properties of the coefficient matrix. In particular, we looked at the special case of banded matrices. We could construct specialty solvers for other classes of coefficient matrix by analyzing the process involved in using the *Gaussian Elimination* approach to solving the corresponding system. Taking an algorithmic approach is much slower for a single problem, but pays off because we end up with a constructed routine that can \"quickly\" solve any problem fitting the assumptions of the algorithm we've built."],"metadata":{"id":"zYLVApyRkl1_"}},{"cell_type":"code","source":[],"metadata":{"id":"a3camZaJlZhg"},"execution_count":null,"outputs":[]}]}