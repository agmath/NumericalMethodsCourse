{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6oo2qLbX1bwce3ZkALugW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Day 2: Binary Representation and Motivating Examples\n","\n","We concluded our *Day 1* notebook by seeing that Python does not recognize that `0.1 + 0.2` is equal to `0.3`. We certainly recognize the equality, but why doesn't Python?"],"metadata":{"id":"OKdgI9rGWLRb"}},{"cell_type":"markdown","source":["## Binary Representations of Integers\n","\n","Humans evolved to use a base-10 number system simply because we have 10 fingers and 10 toes to count on. Computers don't have fingers or toes and can only \"think\" in terms of electrical pulses. For this reason, work in binary -- you can think of this as $0$ indicating a low voltage value and $1$ indicating a high voltage value.\n","\n","Since computers work in *binary*, they don't natively understand our base-10 number system. All information stored in a computer must be stored in binary. Luckily, all numbers have a corresponding binary representation. Below we show how we can use the *Euclidean Algorithm* to convert an integer ($84$) into its binary representation.\n","\n","\\begin{align*} 84 &= 2\\left(42\\right) + 0\\\\\n","42 &= 2\\left(21\\right) + 0\\\\\n","21 &= 2\\left(10\\right) + 1\\\\\n","10 &= 2\\left(5\\right) + 1\\\\\n","5 &= 2\\left(2\\right) + 1\\\\\n","2 &= 2\\left(1\\right) + 0\\\\\n","1 &= 2\\left(0\\right) + 1\n","\\end{align*}\n","\n","Reading the remainders from bottom to top, we find that the binary representation of $\\left(84\\right)_{10} = \\left(1011100\\right)_{2}$. Each *bi*nary dig*it* is referred to as a *bit*, and it is common to consider collections of $8$ bits together -- known as a *byte*. That is, we would more commonly write $\\left(01011100\\right)_{2}$ as the binary representation of $84$."],"metadata":{"id":"BLWfpGdPXPMs"}},{"cell_type":"markdown","source":["**Example:** Use the *Euclidean Algorithm* to find the binary representation of $\\left(118\\right)_{10}$."],"metadata":{"id":"AbzrJXCtdnOQ"}},{"cell_type":"markdown","source":["There is more to the story with binary representations of integers. For example, there are several different representation schemes allowing for representation of positive, $0$, and negative integers. Three popular schemes are *sign-and-magnitude*, *ones-complement*, and *twos-complement*. We'll be more interested in *floating point* numbers than in integers though, so we'll leave our discussion on integer representations here."],"metadata":{"id":"_hidEUcPevqz"}},{"cell_type":"markdown","source":["## Binary Representations of Real Numbers\n","\n","It is also posible to represent non-integer values using binary. For example, $\\frac{1}{8} = 0.125$ can be represented as $\\left(0.001\\right)_{2}$ with the justification shown below.\n","\n","\\begin{align*} 2\\left(0.125\\right) &= 0.25 + 0\\\\\n","2\\left(0.25\\right) &= 0.5 + 0\\\\\n","2\\left(0.5\\right) &= 0 + 1\n","\\end{align*}\n","\n","Collecting the integer components from top to bottom, we see that $0.125 = \\left(0.001\\right)_{2}$.\n","\n","We'll show one example of how to convert another real number into binary.\n","\n","**Example:** Convert $53.7$ to binary.\n","> *Solution.* We'll convert the integer and decimal parts separately. We begin by converting $53$ to binary.\n","\\begin{align*} 53 &= 2\\left(26\\right) + 1\\\\\n","26 &= 2\\left(13\\right) + 0\\\\\n","13 &= 2\\left(6\\right) + 1\\\\\n","6 &= 2\\left(3\\right) + 0\\\\\n","3 &= 2\\left(1\\right) + 1\\\\\n","1 &= 2\\left(0\\right) + 1\n","\\end{align*}\n","> So the binary representation of $53$ is $\\left(110101\\right)_{2}$. Now we'll convert $0.7$ into binary.\n","\\begin{align*} 2\\left(0.7\\right) &= 0.4 + 1\\\\\n","2\\left(0.4\\right) &= 0.8 + 0\\\\\n","2\\left(0.8\\right) &= 0.6 + 1\\\\\n","2\\left(0.6\\right) &= 0.2 + 1\\\\\n","2\\left(0.2\\right) &= 0.4 + 0\\\\\n","\\vdots~~~~ &= ~~~~~\\vdots\n","\\end{align*}\n","> Notice that, since we've arrived at a decimal of $0.4$ again, we will have an infinite repeating pattern of $0110$ following the first $1$. That is, $0.7 = \\left(101100110\\cdots 0110\\cdots\\right)_{2}$.\n",">\n","> Putting these two pieces together, we have that $53.7 = \\left(110101.101100110\\cdots\\right)_{2}$. Note that the \"decimal point\" in bases other than $10$ is commonly referred to as the ***radix***.\n","\n","**Example:** Convert the real number $38.75$ into its binary representation."],"metadata":{"id":"8_OnKciNb1tW"}},{"cell_type":"markdown","source":["Now that we know that real numbers can be represented in binary, we'll move on to how computers store *floating point* numbers."],"metadata":{"id":"jCztF4txwziX"}},{"cell_type":"markdown","source":["## Floating Point Representation\n","\n","Floating point numbers can be represented with *single precision* (32-bits) or *double precision* (64-bits). A floating point representation consists of a *mantissa* (significant digits) and an *exponent* (magnitude). The standard for representing floating point numbers is identified in [IEEE 754](https://youtu.be/RuKkePyo9zk)\n","\n","+ For *single precision*...\n","\n","  + One sign bit (0 is positive, 1 is negative)\n","  + An 8-bit *exponent* (a bias of $127$ is subtracted from the exponent to allow for exponents ranging from $-126$ to $127$). An exponent of $0$ is reserved for special cases such as $0$ or `NaN`.\n","  + A 23-bit *mantissa* which determines the precision of the number\n","\n","    + The mantissa is normalized so that the leftmost bit is always a $1$ and there is only a single $1$ to the left of the radix. This allows for a free extra bit of precision since that $1$ does not need to be stored.\n","\n","+ For *double precision*...\n","\n","  + One sign bit is used\n","  + An 11-bit *exponent* is used and the bias is $1023$, which allows for exponents between $-1022$ and $1023$.\n","  + A 52-bit *mantissa* determines the precision of the number.\n","\n","The range for *single-precision* floats is approximately between $\\pm 1.2\\times 10^{-38}$ and $\\pm 3.4\\times 10^{38}$ with a precision of around $7$ decimal places. Similarly, the range for *double-precision* floats is approximately between $\\pm 2.2\\times 10^{-308}$ and $\\pm 1.8\\times 10^{308}$."],"metadata":{"id":"isw8WokwfYOg"}},{"cell_type":"markdown","source":["**Example:** Computers represent the integer $5$ in double precision as follows:\n","> Since $5 = \\left(101\\right)_{2}$, we can represent $5$ in double precision as $1.\\underbrace{0100\\cdots 0}_{\\text{52 bits}}\\times 2^2$ or $1.\\underbrace{0100\\cdots 0}_{\\text{52 bits}}\\times 2^\\underbrace{100\\cdots 01}_{\\text{11 bits}}$ (remember that the exponent has a bias adjustment of $1023$). According to IEEE 754, the number $5$ is represented in double-precision by\n","$$\\overbrace{0}^{\\text{sign}}\\underbrace{100\\cdots 01}_{\\text{Exponent, 11 bits}}\\overbrace{0100\\cdots 0}^{\\text{mantissa, 52 bits}}$$"],"metadata":{"id":"n13bMaxNhmsI"}},{"cell_type":"markdown","source":["**Remark:** The real numbers $0.1$, $0.2$, and $0.3$ are all non-terminating decimals in binary. This is the reason that `0.1 + 0.2 != 0.3` -- the rounding errors in the floating point representations are the issue!"],"metadata":{"id":"puWfpemAxLcY"}},{"cell_type":"markdown","source":["**Example (Machine Epsilon):** Machine epsilon is the smallest size step you can take from the number $1$. In double precision, that number is $2^{-53}$ since the *mantissa* contains $52$ precision bits, plus the extra precision due to normalization.\n","\n","*Question:* What is machine epsilon for a single precision float?"],"metadata":{"id":"yUgUccuAhn_4"}},{"cell_type":"code","source":[],"metadata":{"id":"bgeqOjKsd9Tw"},"execution_count":null,"outputs":[]}]}