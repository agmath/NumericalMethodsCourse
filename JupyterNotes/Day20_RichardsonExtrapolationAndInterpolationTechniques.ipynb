{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNw+Dt/Q/FCh8gas7e0MxFM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nC56JWaBDuL6","cellView":"form"},"outputs":[],"source":["# @title\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"markdown","source":["# Day 20: Richardson Extrapolation and Derivatives by Interpolation\n","\n","In this notebook we consider two additional techniques. The first, Richardson Extrapolation, is a general method which can be used to improve the accuracy of an approximation provided that the dominant error term takes the form $E\\left(h\\right) = ch^p$, where $c$ and $p$ are constants. The second additional technique uses *interpolation* methods to approximate the derivative of $f\\left(x\\right)$ in cases where only a discrete set of observed values satisfying $f\\left(x\\right)$ are known."],"metadata":{"id":"2tvmSlXUdjN0"}},{"cell_type":"markdown","source":["## Richardson Extrapolation\n","\n","Richardson Extrapolation is a general class of techniques for increasing the accuracy of some numerical procedures -- including finite difference methods. The general strategy appears below.\n","\n","Suppose that we have a strategy for approximating a quantity $G$ and that the resulting approximation for $G$ depends on a parameter $h$. We can write that approximation by $G = g\\left(h\\right) + E\\left(h\\right)$, where $E\\left(h\\right)$ is the approximation error at $h$. Richardson Extrapolation attempts to remove the error, provided that it has the form $E\\left(h\\right) = ch^p$ with $c$ and $p$ as constants. If this is the case, we have\n","\n","\\begin{align*}\n","G &= g\\left(h_1\\right) + E\\left(h_1\\right)\\\\\n","\\implies G &= g\\left(h_1\\right) + ch_1^p\n","\\end{align*}\n","\n","We can do the same with several different values of the parameter $h$ to obtain the system:\n","\n","$$\\left\\{\\begin{array}{lcl} G & = & g\\left(h_1\\right) + ch_1^p\\\\\n","G & = & g\\left(h_2\\right) + ch_2^p\n","\\end{array}\\right.$$\n","\n","We can multiply the top equation by $h_2^p$ and the bottom equation by $h_1^p$, and then subtract to eliminate $c$.\n","\n","\\begin{align*} G\\cdot h_1^p - G\\cdot h_2^p &= \\left(h_1^p\\cdot g\\left(h_2\\right) + ch_1^ph_2^p\\right) - \\left(h_2^p\\cdot g\\left(h_1\\right) + ch_1^ph_2^p\\right)\\\\\n","\\implies G\\cdot h_1^p - Gh_2^p &= h_1^p\\cdot g\\left(h_2\\right) - h_2^p\\cdot g\\left(h_1\\right)\\\\\n","G\\left(h_1^p - h_2^p\\right) &= h_1^p\\cdot g\\left(h_2\\right) - h_2^p\\cdot g\\left(h_1\\right)\\\\\n","\\implies G &= \\frac{h_1^p\\cdot g\\left(h_2\\right) - h_2^p\\cdot g\\left(h_1\\right)}{h_1^p - h_2^p}\\\\\n","\\implies G &= \\frac{h_2^p\\left(\\frac{h_1^p}{h_2^p}\\cdot g\\left(h_2\\right) - g\\left(h_1\\right)\\right)}{h_2^p\\left(\\frac{h_1^p}{h_2^p} - 1\\right)}\\\\\n","\\implies G &= \\frac{\\left(\\frac{h_1}{h_2}\\right)^p\\cdot g\\left(h_2\\right) - g\\left(h_1\\right)}{\\left(\\frac{h_1}{h_2}\\right)^p - 1}\n","\\end{align*}\n","\n","If we use $h_2 = h_1/2$, then the last line above becomes\n","\n","$$G = \\frac{2^p\\cdot g\\left(h_1/2\\right) - g\\left(h_1\\right)}{2^p - 1}$$\n","\n","Notice now that the error term has been eliminated! In the previous notebook, we examined the finite difference method to approximate $f''\\left(x\\right)$ for $f\\left(x\\right) = e^{-x}$ at $x = 1$. Let's do the same here, but utilize Richardson Extrapolation to improve our approximation. Note that since our truncation error when we did this in the previous notebook was $\\mathscr{O}\\left(h^2\\right)$, we have $p = 2$."],"metadata":{"id":"g_w5bwdRDzUi"}},{"cell_type":"markdown","source":["**Example:** Use the first centered finite difference approximation in conjunction with Richardson Extrapolation to approximate $f'\\left(x\\right)$ for $f\\left(x\\right) = e^{-x}$ at $x = 1$."],"metadata":{"id":"9d7l0nhRNlbh"}},{"cell_type":"code","source":["def f(x):\n","  return np.exp(-x)\n","\n","h = np.array([0.64, 0.32, 0.16, 0.08, 0.04, 0.02, 0.01, 0.005, 0.0025, 0.00125])\n","x = 1.0\n","\n","est_6d = np.round((np.round(f(x + h), 6) - 2*np.round(f(x), 6) + np.round(f(x - h), 6))/np.round(h**2, 6), 6)\n","est_8d = np.round((np.round(f(x + h), 8) - 2*np.round(f(x), 8) + np.round(f(x - h), 8))/np.round(h**2, 8), 8)\n","\n","results_df = pd.DataFrame({\"h\": h, \"Six-Digit Precision\" : est_6d, \"Eight-Digit Precision\" : est_8d})\n","\n","p = 2\n","\n","results_df[\"Richardson Extrapolation (6-digit)\"] = (2**p*results_df[\"Six-Digit Precision\"].shift(-1) - results_df[\"Six-Digit Precision\"])/(2**p - 1)\n","results_df[\"Richardson Extrapolation (8-digit)\"] = (2**p*results_df[\"Eight-Digit Precision\"].shift(-1) - results_df[\"Eight-Digit Precision\"])/(2**p - 1)\n","\n","results_df"],"metadata":{"id":"hIr-0i7UDy5r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you remember, the value of $f''\\left(1\\right)$ for $f\\left(x\\right) = e^{-x}$ is near $0.367879$. We obtain an estimate accurate to four decimal places even for quite large $h$. At $h_1 = 0.64$, we are already as accurate as the most accurate eight-digit precision estimate even though we are only using six-digit precision in the fourth column. Notice that using smaller values of $h$ quickly lead to round-off error issues, regardless of the level of precision we are utilizing. This is something to remain aware of!"],"metadata":{"id":"_SN39zPxPguy"}},{"cell_type":"markdown","source":["### More Examples\n","\n","Let's see some additional examples which will enlighten us to additional methods for improving our estimates."],"metadata":{"id":"bCN2DooBR4xM"}},{"cell_type":"markdown","source":["**Example 5.1:** Given the evenly spaced observed data points in the table below, compute $f'\\left(x\\right)$ and $f''\\left(x\\right)$ at $x = 0$ and $x = 0.2$ using finite difference approximations of $\\mathscr{O}\\left(h^2\\right)$.\n","\n","<center>\n","\n","$x$ | $y$\n","--- | ---\n","0 | 0.0000\n","0.1 | 0.0819\n","0.2 | 0.1341\n","0.3 | 0.1646\n","0.4 | 0.1797\n","\n","\n","</center>\n","\n","> *Solution.*"],"metadata":{"id":"ord0xSLoSBIM"}},{"cell_type":"markdown","source":["**Example 5.2:** Use the data in ***Example 5.1*** to estimate $f'\\left(0\\right)$ as accurately as possible.\n","\n","> *Solution.*"],"metadata":{"id":"XAvy9wnbSTXb"}},{"cell_type":"markdown","source":["**Example 5.3:** The linkage shown below has the dimensions $a = 100mm$, $b = 120mm$, $c = 150mm$, and $d = 180mm$.\n","\n","![linkage](https://drive.google.com/uc?export=view&id=1wmLIOgrRcuYvRR3tQQp5Zoe9Pr5z_DCN)\n","\n","Using geometry, we can show that the relationship between the angles $\\alpha$ and $\\beta$ is\n","\n","<center>\n","\n","$\\alpha$ (degree) | $\\beta$ (radians)\n","--- | ---\n","0 | 1.6595\n","5 | 1.5434\n","10 | 1.4186\n","15 | 1.2925\n","20 | 1.1712\n","25 | 1.0585\n","30 | 0.9561\n","\n","</center>\n","\n","If link $AB$ rotates with constant angular velocity $25~\\text{rad/sec}$, use finite difference approximations of $\\mathcal{O}\\left(h^2\\right)$ to tabulate the angular velocity $\\frac{d\\beta}{dt}$ of link $BC$ against $\\alpha$.\n","\n","> *Solution.*"],"metadata":{"id":"Od3W6SXkS6PM"}},{"cell_type":"markdown","source":["## Derivatives by Interpolation\n","\n","You may have noted that our finite difference techniques require that observed data points be equally spaced. This is perhaps an unrealistic expectation. In cases where observations are recorded along uneven intervals of $x$, then finite difference methods don't apply."],"metadata":{"id":"xdjdkmbFeYWp"}},{"cell_type":"markdown","source":["### Derivatives via Polynomial Interpolants\n","\n","Recall that we know how to fit a polynomial interpolant of degree $n$ through $n+1$ observed data points. That is, we can fit\n","\n","$$P_n\\left(x\\right) = a_0 + a_1x + a_2x^2 + \\cdots + a_nx^n$$\n","\n","Since we can fit this polynomial, we can compute and evaluate any of its derivatives at any desired $x$. As a reminder, we mentioned earlier that keeping the degree of the polynomial low is advisable to avoid our interpolant having wild oscillation between observed points. For this reason, we typically use *local interpolation* when our goal is to approximate a derivative. That is, if we wish to approximate $f^{\\left(k\\right)}\\left(x_0\\right)$, we'll build a polynomial interpolant using only the observed data points nearest $x_0$.\n","\n","**Note:** If observed data points are equally spaced, then polynomial interpolation and finite difference methods will give the same results. Finite difference methods are a special case of polynomial interpolation to approximate derivatives.\n","\n","In order to compute the derivative of our fitted polynomial, we'll need to obtain its coefficients. The only method we have that does this from earlier in our course is the *least squares fitting algorithm*. Recall that using a number of terms equal to the number of observed data points we have, however, results in fitting an interpolant. We can use this technique to approximate our derivative. If we know that our observed data contains noise, then we can fit a polynomial using least squares with a lesser number of terms and use that to approximate our derivatives."],"metadata":{"id":"Ph9mzYROe1RT"}},{"cell_type":"markdown","source":["### Derivatives via Polynomial and Cubic Spline Interpolants\n","\n","We said earlier in our course that, because of its stiffness, a cubic spline is a good *global* interpolant. Additionally, since it is a piecewise cubic function, it is also easy to differentiate!\n","\n","Recall that the second derivatives $k_i$ of the spline at the knot values are computed at the beginning of the process of constructing the spline. We did this with the `curvatures()` function from Day 9. From here, we can compute the first and second derivatives of the spline at any interior $x$ using:\n","\n","\\begin{align*} f_{i, i+1}'\\left(x\\right) &= \\frac{k_i}{6}\\left[\\frac{3\\left(x - x_i\\right)^2}{x_i - x_{i+1}} - \\left(x_i - x_{i+1}\\right)\\right] - \\frac{k_{i+1}}{6}\\left[\\frac{3\\left(x - x_i\\right)^2}{x_i - x_{i+1}} - \\left(x_i - x_{i+1}\\right)\\right] + \\frac{y_i - y_{i+1}}{x_i - x_{i+1}}\\\\\n","f_{i, i+1}''\\left(x\\right) &= k_i\\left(\\frac{x - x_{i+1}}{x_i - x_{i+1}}\\right) - k_{i+1}\\left(\\frac{x - x_i}{x_i - x_{i+1}}\\right)\n","\\end{align*}\n","\n","What concerns might we have in estimating derivatives from polynomial interpolants?\n","\n","Let's work through some examples."],"metadata":{"id":"wfSm9_pAiPRv"}},{"cell_type":"markdown","source":["**Example 5.4a:** Given the observed data below, compute $f'\\left(2\\right)$ and $f''\\left(2\\right)$ using a polynomial interpolant over three points.\n","\n","<center>\n","\n","x | f(x)\n","--- | ---\n","1.5 | 1.0628\n","1.9 | 1.3961\n","2.1 | 1.5432\n","2.4 | 1.7249\n","2.6 | 1.8423\n","3.1 | 2.0397\n","\n","</center>\n","\n","> *Solution.*"],"metadata":{"id":"u_svPmQvkAZ-"}},{"cell_type":"code","source":[],"metadata":{"id":"5url_OrBPrhM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example 5.4b:** Given the observed data below, compute $f'\\left(2\\right)$ and $f''\\left(2\\right)$ using a cubic spline interpolant spanning all of the observed data points.\n","\n","<center>\n","\n","x | f(x)\n","--- | ---\n","1.5 | 1.0628\n","1.9 | 1.3961\n","2.1 | 1.5432\n","2.4 | 1.7249\n","2.6 | 1.8423\n","3.1 | 2.0397\n","\n","</center>\n","\n","> *Solution.*"],"metadata":{"id":"VMehSsi7kiKr"}},{"cell_type":"code","source":[],"metadata":{"id":"R2ybZcFUkoBd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Example 5.5:** The observed data below is known to be *noisy*. Determine $f'\\left(0\\right)$ and $f'\\left(1\\right)$ as best as possible. Consider multiple fits, including quadratic through fourth-order.\n","\n","<center>\n","\n","x | f(x)\n","--- | ---\n","0 | 1.9934\n","0.2 | 2.1465\n","0.4 | 2.2129\n","0.6 | 2.1790\n","0.8 | 2.0683\n","1.0 | 1.9448\n","1.2 | 1.7655\n","1.4 | 1.5891\n","\n","</center>\n","\n","> *Solution.*"],"metadata":{"id":"Y99fiWt_kytd"}},{"cell_type":"code","source":[],"metadata":{"id":"ZbWFt7wjmOTp"},"execution_count":null,"outputs":[]}]}